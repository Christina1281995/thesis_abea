{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation for Trained Model for ATE and AEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, a model path is used as input to \n",
    "1) load a fine-tuned GRACE Model for either ABSA or ABEA\n",
    "2) pre-process the SemEval datasets (restaurant 2014, laptop 2014, and restaurant 2014, 2015 and 2016 union) according to the GRACE format\n",
    "3) run an inference on the datasets\n",
    "4) calculate recall, precision, f1 and accuracy metrics using two calculation methods:\n",
    "    - exact and complete matches only: an inference (a found aspect term and emotion/ sentiment) are only counted as true positive if the start and end indices overlap with those in the labeled dataset\n",
    "    - word-by-word: each word is compared between the labeled dataset and the inference - if a word is identified as an aspect term in both datasets, it is a true positive  \n",
    "\n",
    "The first calculation will only capture a complete match between the inferred label and the label in the dataset, for example if the dataset contains the aspect term ‚Äúfirst course‚Äù, only the complete inference ‚Äúfirst course‚Äù would categorize as true positive, whereas only ‚Äúcourse‚Äù would be a false positive. This calculation is a stricter assessment than the second calculation which assesses matching aspect terms in the model inference and the labeled dataset word for word. In that case the identification of only ‚Äúcourse‚Äù would result in one false negative ‚Äúfirst‚Äù and one true positive ‚Äúcourse‚Äù. This calculation includes partial matches and gives an overview of the amount of aspect term words that were found.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'GRACE/')                                        # Add 'GRACE/' directory at the start of the sys.path for module searching\n",
    "import os                                                           # used to interact with the operating system\n",
    "\n",
    "import torch                                                        # a deep learning framework for tensor computations and automatic differentiation\n",
    "import torch.nn.functional as F                                     # torch.nn.functional is for activation and loss functions in pytorch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset # pytorch classes for data handling, sampling, batching etc.\n",
    "\n",
    "import argparse                                                     # for parsing command line arguments\n",
    "import random                                                       # to generate random numbers\n",
    "import numpy as np                                                  # for numerical and array operations\n",
    "import pandas as pd                                                 # for data manipulation and analysis\n",
    "\n",
    "from tokenization import BertTokenizer                              # for tokenizing text into tokens understandable by BERT\n",
    "\n",
    "# Import local files from the GRACE folder in this repo\n",
    "from ate_asc_modeling_local_bert_file import BertForSequenceLabeling\n",
    "import ate_asc_modeling_local_bert_file                             # local file containing BERT modeling utilities for ATE and ASC\n",
    "from file_utils import PYTORCH_PRETRAINED_BERT_CACHE                # PYTORCH_PRETRAINED_BERT_CACHE to manage caching for pre-trained BERT models\n",
    "from ate_asc_features import ATEASCProcessor, convert_examples_to_features, get_labels\n",
    "\n",
    "from tqdm import tqdm                                               # for progress monitoring\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose which model will be evaluated using the SemEval Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameofrun = 'ABEA_with_0.25DO_0.05_weight_decay'\n",
    "epoch = '8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRACE/data/same_split_as_absa/out_ABEA_with_0.25DO_0.05_weight_decay_ateacs/pytorch_model.bin.8\n"
     ]
    }
   ],
   "source": [
    "# Your device settings\n",
    "device = \"cuda\"                                                 # else \"cpu\"\n",
    "n_gpu = 1                                                       # nr. of GPUs being used\n",
    "\n",
    "# -------------------------------------\n",
    "\n",
    "path = f'GRACE/data/same_split_as_absa/out_{nameofrun}_ateacs/'\n",
    "# path = \"GRACE/data/same_split_as_absa/decoder_fine_tuning/\"\n",
    "model = 'pytorch_model.bin.' + epoch\n",
    "# model = 'pytorch_model.bin.4'\n",
    "\n",
    "# last result from the original settings with all the ABEA training data --USED IN APPLICATION CASE/ USE CASE FOR CALI\n",
    "# model_path = 'GRACE/out_abea_ateacs/pytorch_model.bin.9'\n",
    "# model_path = 'GRACE/data/same_split_as_absa/out_no_vat_ateacs/pytorch_model.bin.7'\n",
    "model_path = path + model\n",
    "print(model_path)\n",
    "\n",
    "\n",
    "# -------------------------------------\n",
    "\n",
    "# for emotions\n",
    "\n",
    "data_directory = 'GRACE/data/same_split_as_absa/'\n",
    "\n",
    "train='nouns_w_none_train.txt'\n",
    "valid='nouns_w_none_trial.txt'\n",
    "test='nouns_w_none_test.gold.txt'\n",
    "\n",
    "\n",
    "\n",
    "# data_directory = 'GRACE/data/'\n",
    "\n",
    "# # train='abea_train.txt'\n",
    "# # valid='abea_trial.txt'\n",
    "# # test='abea_test.gold.txt'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # for sentiments\n",
    "# train='twitter_1_train.txt'\n",
    "# valid='twitter_1_trial.txt'\n",
    "# test='twitter_1_test.gold.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load your GeoDataFrame\n",
    "# tweets_intersecting_fires_gdf = gpd.read_file('Data/Case Study Data/tweets_intersecting_fires.shp')\n",
    "\n",
    "# # Print the CRS and head to verify data\n",
    "# print(tweets_intersecting_fires_gdf.crs)\n",
    "# print(tweets_intersecting_fires_gdf.head())\n",
    "\n",
    "# # Ensure the CRS is set correctly\n",
    "# if tweets_intersecting_fires_gdf.crs is None:\n",
    "#     tweets_intersecting_fires_gdf.set_crs(epsg=3857, inplace=True)\n",
    "\n",
    "# # Convert the CRS to EPSG:4326 to verify coordinates\n",
    "# tweets_intersecting_fires_gdf_geo = tweets_intersecting_fires_gdf.to_crs(epsg=4326)\n",
    "\n",
    "# # Print the head to check the coordinates\n",
    "# print(tweets_intersecting_fires_gdf_geo.head())\n",
    "\n",
    "# # Plot the data without basemap to verify locations\n",
    "# fig, ax = plt.subplots(figsize=(10, 10))\n",
    "# tweets_intersecting_fires_gdf_geo.plot(ax=ax, marker='o', color='red', markersize=5, alpha=0.7, edgecolor='k')\n",
    "# plt.title('Tweets Intersecting Fires (Geographic Coordinates)')\n",
    "# plt.xlabel('Longitude')\n",
    "# plt.ylabel('Latitude')\n",
    "# plt.show()\n",
    "\n",
    "# # Convert back to Web Mercator for plotting with basemap\n",
    "# tweets_intersecting_fires_gdf = tweets_intersecting_fires_gdf_geo.to_crs(epsg=3857)\n",
    "\n",
    "# # Create the plot with basemap\n",
    "# fig, ax = plt.subplots(figsize=(10, 10))\n",
    "# tweets_intersecting_fires_gdf.plot(ax=ax, marker='o', color='red', markersize=5, alpha=0.7, edgecolor='k')\n",
    "\n",
    "# # Add basemap with a fixed zoom level\n",
    "# ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik, zoom=10)\n",
    "\n",
    "# # Set plot title and labels\n",
    "# plt.title('Tweets Intersecting Fires with Basemap')\n",
    "# plt.xlabel('Longitude')\n",
    "# plt.ylabel('Latitude')\n",
    "\n",
    "# # Show plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_emotion_words1(df):\n",
    "    # Flatten the list of emotions into individual words\n",
    "    all_emotions = []\n",
    "    for emotions in df['aspect_emotions']:\n",
    "        for emotion_phrase in emotions:\n",
    "            # Split each phrase by spaces to handle repeated emotions and add to the list\n",
    "            all_emotions.extend(emotion_phrase.split())\n",
    "\n",
    "    # Use Counter to count occurrences of each word\n",
    "    emotion_counts = Counter(all_emotions)\n",
    "\n",
    "    return emotion_counts\n",
    "\n",
    "def count_emotion_words2(df, column):\n",
    "    # Flatten the list of emotions into individual words\n",
    "    all_emotions = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Split each phrase by spaces to handle repeated emotions and add to the list\n",
    "        all_emotions.append(row[column])\n",
    "\n",
    "    # Use Counter to count occurrences of each word\n",
    "    emotion_counts = Counter(all_emotions)\n",
    "\n",
    "    return emotion_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read the GRACE-formatted txt file WITH EMOTIONS into a df with a 'text' and 'label' column.\n",
    "def grace_format_to_df_cols(txt_path):\n",
    "\n",
    "    with open(txt_path, 'r') as file:\n",
    "        file_content = file.read()\n",
    "    \n",
    "    content_without_docstart = file_content.split('\\n', 1)[1]\n",
    "\n",
    "    # Split the content based on empty lines to get each sequence\n",
    "    sequences = [seq.strip() for seq in content_without_docstart.split('\\n\\n') if seq.strip()]\n",
    "\n",
    "    texts_labels = [process_sequence(seq) for seq in sequences]\n",
    "    texts, labels = zip(*texts_labels)  # Unzip the texts and labels\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'text': texts,\n",
    "        'label': labels\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "# function to process each of the sequences\n",
    "def process_sequence(seq):\n",
    "    words = []\n",
    "    labels = []\n",
    "    char_index = 0  # Start from the first character\n",
    "    in_label = False\n",
    "    label_start = 0\n",
    "    label_sentiment = ''\n",
    "\n",
    "    for line in seq.split('\\n'):\n",
    "        parts = line.split()\n",
    "        if len(parts) < 2:  # Skip empty lines or lines without tags\n",
    "            continue\n",
    "        word, tag = parts[0], parts[-1]\n",
    "        words.append(word)\n",
    "\n",
    "        # If this word starts a new label\n",
    "        if 'B_AP' in tag:\n",
    "            if in_label:  # End the previous label if starting a new one\n",
    "                labels.append([label_start, char_index - 1, label_sentiment])\n",
    "            in_label = True\n",
    "            label_start = char_index\n",
    "            # Determine the emotion\n",
    "            if 'FEAR' in tag:\n",
    "                label_sentiment = 'FEAR'\n",
    "            elif 'NONE' in tag:\n",
    "                label_sentiment = 'NONE'\n",
    "            elif 'HAPPINESS' in tag:\n",
    "                label_sentiment = 'HAPPINESS'\n",
    "            elif 'ANGER' in tag:\n",
    "                label_sentiment = 'ANGER'\n",
    "            elif 'SADNESS' in tag:\n",
    "                label_sentiment = 'SADNESS'\n",
    "\n",
    "        # If this word is not part of a label or starts a new label\n",
    "        if 'B_AP' not in tag and 'I_AP' not in tag and in_label:\n",
    "            labels.append([label_start, char_index - 1, label_sentiment])\n",
    "            in_label = False\n",
    "\n",
    "        char_index += len(word) + 1  # Update character index for next word, adding 1 for the space\n",
    "\n",
    "    # If the last label goes till the end of the sequence\n",
    "    if in_label:\n",
    "        labels.append([label_start, char_index - 1, label_sentiment])\n",
    "\n",
    "    return ' '.join(words), labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model with Correct Labels and Training State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set arguments required for model initialization\n",
    "args = argparse.Namespace(\n",
    "    data_dir=data_directory, \n",
    "    bert_model='bert-base-uncased',\n",
    "    init_model=None,\n",
    "    task_name=\"ate_asc\",\n",
    "    data_name=\"abea\",\n",
    "    train_file=train,\n",
    "    valid_file=valid,\n",
    "    test_file=test,\n",
    "    output_dir='out_testing/',\n",
    "    seed = 42,\n",
    "    do_lower_case=True,\n",
    "    local_rank=-1,\n",
    "    eval_batch_size=32,\n",
    "    max_seq_length=128,\n",
    "    use_ghl=True, \n",
    "    use_vat=False, \n",
    "    use_decoder=True, \n",
    "    num_decoder_layer=2, \n",
    "    decoder_shared_layer=3)\n",
    "\n",
    "random.seed(args.seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "n_gpu = 1\n",
    "data_name = args.data_name.lower()\n",
    "task_name = args.task_name.lower()\n",
    "task_config = {\n",
    "    \"use_ghl\": True,\n",
    "    \"use_vat\": False,\n",
    "    \"num_decoder_layer\": 2,\n",
    "    \"decoder_shared_layer\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader to preprocess the input data\n",
    "def dataloader_val(args, tokenizer, file_path, label_tp_list, set_type=\"val\"):\n",
    "\n",
    "    dataset = ATEASCProcessor(file_path=file_path, set_type=set_type)\n",
    "    print(\"Loaded val file: {}\".format(file_path))\n",
    "\n",
    "    eval_features = convert_examples_to_features(dataset.examples, label_tp_list,\n",
    "                                                 args.max_seq_length, tokenizer, verbose_logging=False)\n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "    all_at_label_ids = torch.tensor([f.at_label_id for f in eval_features], dtype=torch.long)\n",
    "    all_as_label_ids = torch.tensor([f.as_label_id for f in eval_features], dtype=torch.long)\n",
    "\n",
    "    all_label_mask = torch.tensor([f.label_mask for f in eval_features], dtype=torch.long)\n",
    "    all_label_mask_X = torch.tensor([f.label_mask_X for f in eval_features], dtype=torch.long)\n",
    "\n",
    "    eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_at_label_ids, all_as_label_ids,\n",
    "                              all_label_mask, all_label_mask_X)\n",
    "\n",
    "    # Run prediction for full data\n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "    return eval_dataloader, eval_data\n",
    "\n",
    "# function to initialise the model using the model file (the .bin file), the correct labels, and settings\n",
    "def load_model(model_file, args, num_tp_labels, task_config, device):\n",
    "    model_file = model_file\n",
    "    if os.path.exists(model_file):\n",
    "        model_state_dict = torch.load(model_file, map_location='cpu')\n",
    "        print(\"Model loaded from %s\", model_file)\n",
    "        model = BertForSequenceLabeling.from_pretrained(args.bert_model, cache_dir=PYTORCH_PRETRAINED_BERT_CACHE / 'distributed_{}'.format(args.local_rank),\n",
    "                                                        state_dict=model_state_dict, num_tp_labels=num_tp_labels,\n",
    "                                                        task_config=task_config)\n",
    "        model.to(device)\n",
    "        print(\"model to device\")\n",
    "    else:\n",
    "        model = None\n",
    "        print(\"model none\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/22/2024 22:12:37 - INFO - tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\chris\\.pytorch_pretrained_bert\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "# load bert tokenizer (bert-base-uncased)\n",
    "tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading task specific labels from GRACE/data/same_split_as_absa/nouns_w_none_train.txt\n",
      "ATE labels: {0: 'O', 1: 'B-AP', 2: 'I-AP'}\n",
      "ASC labels: {0: 'O', 1: 'NONE', 2: 'FEAR', 3: 'SADNESS', 4: 'ANGER', 5: 'HAPPINESS'}\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join(args.data_dir, args.train_file)\n",
    "print(f\"Loading task specific labels from {file_path}\")\n",
    "\n",
    "# the ATEASCProcessor reads data and splits it into corpus and label list for ATE and ASC\n",
    "dataset = ATEASCProcessor(file_path=file_path, set_type=\"train\")\n",
    "at_labels, as_labels = get_labels(dataset.label_tp_list)\n",
    "label_tp_list = (at_labels, as_labels)\n",
    "\n",
    "at_num_labels = len(label_tp_list[0])\n",
    "as_num_labels = len(label_tp_list[1])\n",
    "num_tp_labels = (at_num_labels, as_num_labels)\n",
    "\n",
    "task_config[\"at_labels\"] = label_tp_list[0]\n",
    "\n",
    "at_label_list, as_label_list = label_tp_list\n",
    "at_label_map = {i: label for i, label in enumerate(at_label_list)}\n",
    "as_label_map = {i: label for i, label in enumerate(as_label_list)}\n",
    "\n",
    "# print infos to double-check all is correct\n",
    "print(f\"ATE labels: {at_label_map}\")\n",
    "print(f\"ASC labels: {as_label_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from %s GRACE/data/same_split_as_absa/out_ABEA_with_0.25DO_0.05_weight_decay_ateacs/pytorch_model.bin.8\n",
      "loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at C:\\Users\\chris\\.pytorch_pretrained_bert\\distributed_-1\\9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "extracting archive file C:\\Users\\chris\\.pytorch_pretrained_bert\\distributed_-1\\9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir C:\\Users\\chris\\AppData\\Local\\Temp\\tmp74zwbpuq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/22/2024 22:12:41 - INFO - decoder_module -   loading archive file c:\\Users\\chris\\Documents\\PLUS SharePoint\\Universit√§t Salzburg\\AG_Geosocial Analytics Lab - MA - Christina Zorenb√∂hmer\\1 - Code\\ABEA Repo\\GRACE\\decoder-bert-base\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model to device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceLabeling(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (weighted_ce_loss_fct): WeightedCrossEntropy()\n",
       "  (decoder): DecoderBertModel(\n",
       "    (embeddings): BertDecoderEmbeddings(\n",
       "      (decoder_word_embeddings): Embedding(3, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): DecoderBertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): BertDecoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x DecoderLayer(\n",
       "          (slf_attn): DecoderAttention(\n",
       "            (att): MultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): DecoderBertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (enc_attn): DecoderAttention(\n",
       "            (att): MultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): DecoderBertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): DecoderBertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder_classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an adapted ate_asc_modeling_local_bert_file.py is imported here, which loads the model from a folder in the repo ('GRACE/bert-base-uncased/bert-base-uncased.tar.gz')\n",
    "\n",
    "model_file = model_path\n",
    "model = load_model(model_file, args, num_tp_labels, task_config, device)\n",
    "\n",
    "if hasattr(model, 'module'):\n",
    "    print('has module')\n",
    "    model = model.module\n",
    "\n",
    "# set model to eval mode (turn off training features e.g. dropout)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model on ABEA Training Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1834\n"
     ]
    }
   ],
   "source": [
    "# absa needs to be read properly from the GRACE formatting so that it can be converted to a df\n",
    "training_data_preformatted = \"GRACE/data/same_split_as_absa/abea_w_none_clean_train.txt\"\n",
    "abea_train = grace_format_to_df_cols(training_data_preformatted)\n",
    "\n",
    "# turn into string format\n",
    "# abea_train['label'] = abea_train['label'].astype(str)\n",
    "# print(abea_train['label'].dtype)\n",
    "\n",
    "print(len(abea_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This guy is bloody hilarious</td>\n",
       "      <td>[[5, 8, HAPPINESS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kremlin critic Navalny says he is not afraid t...</td>\n",
       "      <td>[[15, 22, NONE]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chinook chuntering around outside !</td>\n",
       "      <td>[[0, 18, NONE]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Directors Cut From My Tv Commercial Fire As He...</td>\n",
       "      <td>[[22, 35, HAPPINESS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Woolwich in the MUD #ArtetaIN</td>\n",
       "      <td>[[0, 8, ANGER]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@jadiegray @latsondheimer A combination of bot...</td>\n",
       "      <td>[[28, 39, NONE], [54, 58, SADNESS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@Rovers @UmbroUK @RecoveriteAu Not sure the 94...</td>\n",
       "      <td>[[55, 60, NONE]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                       This guy is bloody hilarious   \n",
       "1  Kremlin critic Navalny says he is not afraid t...   \n",
       "2                Chinook chuntering around outside !   \n",
       "3  Directors Cut From My Tv Commercial Fire As He...   \n",
       "4                      Woolwich in the MUD #ArtetaIN   \n",
       "5  @jadiegray @latsondheimer A combination of bot...   \n",
       "6  @Rovers @UmbroUK @RecoveriteAu Not sure the 94...   \n",
       "\n",
       "                                 label  \n",
       "0                  [[5, 8, HAPPINESS]]  \n",
       "1                     [[15, 22, NONE]]  \n",
       "2                      [[0, 18, NONE]]  \n",
       "3                [[22, 35, HAPPINESS]]  \n",
       "4                      [[0, 8, ANGER]]  \n",
       "5  [[28, 39, NONE], [54, 58, SADNESS]]  \n",
       "6                     [[55, 60, NONE]]  "
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abea_train.loc[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded val file: GRACE/data/same_split_as_absa/abea_w_none_clean_train.txt\n"
     ]
    }
   ],
   "source": [
    "training_data_preformatted = \"GRACE/data/same_split_as_absa/abea_w_none_clean_train.txt\"\n",
    "\n",
    "DATALOADER_DICT = {}\n",
    "# only \"eval\" state is needed (\"train\" is left out)\n",
    "DATALOADER_DICT[\"ate_asc\"] = {\"eval\":dataloader_val}\n",
    "# Create dataloader with the prepared dataset\n",
    "eval_dataloader, eval_examples = DATALOADER_DICT[task_name][\"eval\"](args, tokenizer, training_data_preformatted, label_tp_list=label_tp_list, set_type=\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Model on the Prepared Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58/58 [00:08<00:00,  6.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# empty lists for both the identified aspect terms and the related emotions\n",
    "pred_aspect_terms = []\n",
    "pred_aspect_emotions = []\n",
    "\n",
    "# for-loop to iterate over the preprocessed outputs from the \"eval_dataloader\" \n",
    "# for input_ids, input_mask, segment_ids, at_label_ids, as_label_ids, label_mask, label_mask_X in eval_dataloader:\n",
    "for input_ids, input_mask, segment_ids, at_label_ids, as_label_ids, label_mask, label_mask_X in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    at_label_ids = at_label_ids.to(device)\n",
    "    as_label_ids = as_label_ids.to(device)\n",
    "    label_mask = label_mask.to(device)\n",
    "    label_mask_X = label_mask_X.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # logits, decoder_logits = model(input_ids, segment_ids, input_mask)\n",
    "        logits, sequence_output, encoder_output = model.get_encoder_logits(input_ids, segment_ids, input_mask)\n",
    "        pred_dec_ids = torch.argmax(F.log_softmax(logits, dim=2), dim=2)\n",
    "        decoder_logits = model.get_decoder_logits(encoder_output, input_mask, label_mask_X, pred_dec_ids)\n",
    "        logits = torch.argmax(F.log_softmax(logits, dim=2), dim=2)\n",
    "        decoder_logits = torch.argmax(F.log_softmax(decoder_logits, dim=2), dim=2)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        decoder_logits = decoder_logits.detach().cpu().numpy()\n",
    "        \n",
    "    at_label_ids = at_label_ids.to('cpu').numpy()\n",
    "    as_label_ids = as_label_ids.to('cpu').numpy()\n",
    "    label_mask = label_mask.to('cpu').numpy()\n",
    "    \n",
    "    for i, mask_i in enumerate(label_mask):\n",
    "        temp_1 = []\n",
    "        temp_2 = []\n",
    "        for j, l in enumerate(mask_i):\n",
    "            if l > -1:\n",
    "                temp_1.append(at_label_map[logits[i][j]])\n",
    "                temp_2.append(as_label_map[decoder_logits[i][j]])\n",
    "                \n",
    "        pred_aspect_terms.append(temp_1)\n",
    "        pred_aspect_emotions.append(temp_2)\n",
    "\n",
    "# add new aspect term labels and aspect sentiment labels as columns to twemlab dataframe\n",
    "abea_train['aspect_term_preds'] = pred_aspect_terms\n",
    "abea_train['aspect_emo_preds'] = pred_aspect_emotions\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Next step, extract the aspect term and sentiment \n",
    "aspect_terms = []\n",
    "aspect_sentiments = []\n",
    "\n",
    "# for every row (tweet)\n",
    "for idx, row in abea_train.iterrows():\n",
    "    \n",
    "    row_aspect_terms = []\n",
    "    row_aspect_sentiments = []\n",
    "\n",
    "    # split text into individual words\n",
    "    words = row['text'].split()\n",
    "\n",
    "    # Loop through aspect_term_preds\n",
    "    for i, pred in enumerate(row['aspect_term_preds']):\n",
    "        if pred == 'B-AP':\n",
    "            # Initialize term with the current word\n",
    "            term = words[i] if i < len(words) else ''\n",
    "            sent = row['aspect_emo_preds'][i]\n",
    "\n",
    "            # check for continuation of aspect term ('I-AP')\n",
    "            j = i + 1\n",
    "            while j < len(row['aspect_term_preds']) and row['aspect_term_preds'][j] == 'I-AP':\n",
    "                # Append word to term if within bounds\n",
    "                if j < len(words):\n",
    "                    term += ' ' + words[j]\n",
    "                    sent += ' ' + row['aspect_emo_preds'][j]\n",
    "                j += 1\n",
    "\n",
    "            # save the aspect term and emotion\n",
    "            if term:  # Only add non-empty terms\n",
    "                row_aspect_terms.append(term)\n",
    "                row_aspect_sentiments.append(sent)\n",
    "\n",
    "    # append extracted terms and sentiments to respective lists\n",
    "    aspect_terms.append(row_aspect_terms)\n",
    "    aspect_sentiments.append(row_aspect_sentiments)\n",
    "\n",
    "# assign the lists back to the DataFrame\n",
    "abea_train['aspect_terms'] = aspect_terms\n",
    "abea_train['aspect_emotions'] = aspect_sentiments\n",
    "\n",
    "# remove the previous columns\n",
    "abea_train.drop(columns=['aspect_term_preds', 'aspect_emo_preds'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>aspect_terms</th>\n",
       "      <th>aspect_emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>Just a \" Casual Stroll \" with the dogs and hus...</td>\n",
       "      <td>[[9, 22, HAPPINESS], [64, 68, HAPPINESS]]</td>\n",
       "      <td>[Casual Stroll, dogs, husband, feet]</td>\n",
       "      <td>[NONE NONE, NONE, NONE, NONE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>LACoFD- 5 | Antelope Valley Structure Fire rep...</td>\n",
       "      <td>[[28, 42, NONE]]</td>\n",
       "      <td>[Structure Fire]</td>\n",
       "      <td>[FEAR FEAR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>@BletchleyBark @GuruGryff Hoomum currently sea...</td>\n",
       "      <td>[[26, 32, NONE], [53, 69, NONE]]</td>\n",
       "      <td>[electric fencing]</td>\n",
       "      <td>[FEAR FEAR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>Can ' t wait üôå</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1810</th>\n",
       "      <td>@AP #CrossfireHurricane .</td>\n",
       "      <td>[[4, 23, NONE]]</td>\n",
       "      <td>[#CrossfireHurricane]</td>\n",
       "      <td>[FEAR]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "643   Just a \" Casual Stroll \" with the dogs and hus...   \n",
       "332   LACoFD- 5 | Antelope Valley Structure Fire rep...   \n",
       "990   @BletchleyBark @GuruGryff Hoomum currently sea...   \n",
       "538                                      Can ' t wait üôå   \n",
       "1810                          @AP #CrossfireHurricane .   \n",
       "\n",
       "                                          label  \\\n",
       "643   [[9, 22, HAPPINESS], [64, 68, HAPPINESS]]   \n",
       "332                            [[28, 42, NONE]]   \n",
       "990            [[26, 32, NONE], [53, 69, NONE]]   \n",
       "538                                          []   \n",
       "1810                            [[4, 23, NONE]]   \n",
       "\n",
       "                              aspect_terms                aspect_emotions  \n",
       "643   [Casual Stroll, dogs, husband, feet]  [NONE NONE, NONE, NONE, NONE]  \n",
       "332                       [Structure Fire]                    [FEAR FEAR]  \n",
       "990                     [electric fencing]                    [FEAR FEAR]  \n",
       "538                                     []                             []  \n",
       "1810                 [#CrossfireHurricane]                         [FEAR]  "
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abea_train.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Precision, Recall, f1 on the Restaurants 2014 Dataset, on which the trained GRACE ABEA model has run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# def extract_terms(text, indices):\n",
    "#     terms = []\n",
    "#     for index in indices:\n",
    "#         start, end = index[0], index[1]\n",
    "#         terms.append(text[start:end].strip())\n",
    "#     return terms\n",
    "\n",
    "# def calculate_metrics(df):\n",
    "#     TP = 0\n",
    "#     FP = 0\n",
    "#     FN = 0\n",
    "#     partial_matches = 0\n",
    "    \n",
    "#     for index, row in df.iterrows():\n",
    "#         text = row['text']\n",
    "#         labels = row['label']\n",
    "#         ground_truth_terms = set([extract_terms(text, [label[:2]])[0] for label in labels if label]) if labels else set()\n",
    "        \n",
    "#         predicted_terms = set(row['aspect_terms'])\n",
    "\n",
    "#         # Count True Positives (exact match)\n",
    "#         TP += len(ground_truth_terms.intersection(predicted_terms))\n",
    "        \n",
    "#         # Count False Positives\n",
    "#         FP += len(predicted_terms - ground_truth_terms)\n",
    "        \n",
    "#         # Count False Negatives\n",
    "#         FN += len(ground_truth_terms - predicted_terms)\n",
    "\n",
    "#         # Optionally handle partial matches\n",
    "#         for pred_term in predicted_terms:\n",
    "#             for gt_term in ground_truth_terms:\n",
    "#                 if pred_term in gt_term or gt_term in pred_term:\n",
    "#                     partial_matches += 1\n",
    "\n",
    "#     # Calculate precision, recall, and F1 score\n",
    "#     precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "#     recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "#     f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "#     print(f\"TP: {TP}\")\n",
    "#     print(f\"FP: {FP}\")\n",
    "#     print(f\"FN {FN}\")\n",
    "#     print(f\"Partial Matches: {partial_matches}\")\n",
    "#     return {\n",
    "#         'Precision': precision,\n",
    "#         'Recall': recall,\n",
    "#         'F1 Score': f1,\n",
    "#         'Partial Matches': partial_matches\n",
    "#     }\n",
    "\n",
    "# metrics = calculate_metrics(df_rest14)\n",
    "# print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_words(text, indices):\n",
    "#     words = []\n",
    "#     for index in indices:\n",
    "#         start, end = index[0], index[1]\n",
    "#         # Split the extracted term into words and extend the list\n",
    "#         words.extend(text[start:end].strip().split())\n",
    "#     return words\n",
    "\n",
    "# def calculate_metrics(df):\n",
    "#     TP = 0\n",
    "#     FP = 0\n",
    "#     FN = 0\n",
    "    \n",
    "#     for index, row in df.iterrows():\n",
    "#         text = row['text']\n",
    "#         labels = row['label']\n",
    "#         # Extract all words from the ground truth terms\n",
    "#         ground_truth_words = set(extract_words(text, [label[:2] for label in labels if label])) if labels else set()\n",
    "#         # print(f\"Labels: {ground_truth_words}\")\n",
    "\n",
    "#         # Extract all words from the predicted terms\n",
    "#         predicted_words = set()\n",
    "#         for term in row['aspect_terms']:\n",
    "#             predicted_words.update(term.split())\n",
    "#         # print(f\"Predictions: {predicted_words}\")\n",
    "\n",
    "#         # Count True Positives (exact word match)\n",
    "#         if not ground_truth_words and not predicted_words:\n",
    "#             TP += 1  # Correctly predicted no terms\n",
    "#             # print(\"TP: no aspect terms\")\n",
    "#         else:\n",
    "#             TP += len(ground_truth_words.intersection(predicted_words))\n",
    "#             # print(f\"TP: {ground_truth_words.intersection(predicted_words)}\")\n",
    "\n",
    "#         # Count False Positives\n",
    "#         FP += len(predicted_words - ground_truth_words)\n",
    "#         # print(f\"FP: {predicted_words - ground_truth_words}\")\n",
    "\n",
    "#         # Count False Negatives\n",
    "#         FN += len(ground_truth_words - predicted_words)\n",
    "#         # print(f\"FN: {ground_truth_words - predicted_words}\")\n",
    "\n",
    "#         # print(\"-------\")\n",
    "        \n",
    "#     # Calculate precision, recall, and F1 score\n",
    "#     precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "#     recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "#     f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "#     return {\n",
    "#         'Precision': round(precision, 4),\n",
    "#         'Recall':  round(recall, 4),\n",
    "#         'F1 Score': round(f1, 4)\n",
    "#     }\n",
    "\n",
    "# # Assuming 'df' is already defined and contains the columns 'text', 'label', and 'aspect_terms'\n",
    "# metrics_rest14 = calculate_metrics(df_rest14)\n",
    "# print(metrics_rest14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATE Validation: Laptops 2014 SemEval Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read the GRACE-formatted txt file into a df with a 'text' and 'label' column.\n",
    "def grace_format_to_df_cols(txt_path):\n",
    "\n",
    "    with open(txt_path, 'r') as file:\n",
    "        file_content = file.read()\n",
    "    \n",
    "    content_without_docstart = file_content.split('\\n', 1)[1]\n",
    "\n",
    "    # Split the content based on empty lines to get each sequence\n",
    "    sequences = [seq.strip() for seq in content_without_docstart.split('\\n\\n') if seq.strip()]\n",
    "\n",
    "    texts_labels = [process_sequence(seq) for seq in sequences]\n",
    "    texts, labels = zip(*texts_labels)  # Unzip the texts and labels\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'text': texts,\n",
    "        'label': labels\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "# function to process each of the sequences\n",
    "def process_sequence(seq):\n",
    "    words = []\n",
    "    labels = []\n",
    "    char_index = 0  # Start from the first character\n",
    "    in_label = False\n",
    "    label_start = 0\n",
    "    label_sentiment = ''\n",
    "\n",
    "    for line in seq.split('\\n'):\n",
    "        parts = line.split()\n",
    "        if len(parts) < 2:  # Skip empty lines or lines without tags\n",
    "            continue\n",
    "        word, tag = parts[0], parts[-1]\n",
    "        words.append(word)\n",
    "\n",
    "        # If this word starts a new label\n",
    "        if 'B_AP' in tag:\n",
    "            if in_label:  # End the previous label if starting a new one\n",
    "                labels.append([label_start, char_index - 1, label_sentiment])\n",
    "            in_label = True\n",
    "            label_start = char_index\n",
    "            # Determine the sentiment\n",
    "            if 'NEUTRAL' in tag:\n",
    "                label_sentiment = 'Neutral'\n",
    "            elif 'POSITIVE' in tag:\n",
    "                label_sentiment = 'Positive'\n",
    "            elif 'NEGATIVE' in tag:\n",
    "                label_sentiment = 'Negative'\n",
    "                \n",
    "        # If this word is not part of a label or starts a new label\n",
    "        if 'B_AP' not in tag and 'I_AP' not in tag and in_label:\n",
    "            labels.append([label_start, char_index - 1, label_sentiment])\n",
    "            in_label = False\n",
    "\n",
    "        char_index += len(word) + 1  # Update character index for next word, adding 1 for the space\n",
    "\n",
    "    # If the last label goes till the end of the sequence\n",
    "    if in_label:\n",
    "        labels.append([label_start, char_index - 1, label_sentiment])\n",
    "\n",
    "    return ' '.join(words), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3845\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Boot time is super fast , around anywhere from...</td>\n",
       "      <td>[[0, 9, Positive]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tech support would not fix the problem unless ...</td>\n",
       "      <td>[[0, 12, Negative]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>but in resume this computer rocks !</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Set up was easy .</td>\n",
       "      <td>[[0, 6, Positive]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Did not enjoy the new Windows 8 and touchscree...</td>\n",
       "      <td>[[22, 31, Negative], [36, 57, Negative]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Boot time is super fast , around anywhere from...   \n",
       "1  tech support would not fix the problem unless ...   \n",
       "2                but in resume this computer rocks !   \n",
       "3                                  Set up was easy .   \n",
       "4  Did not enjoy the new Windows 8 and touchscree...   \n",
       "\n",
       "                                      label  \n",
       "0                        [[0, 9, Positive]]  \n",
       "1                       [[0, 12, Negative]]  \n",
       "2                                        []  \n",
       "3                        [[0, 6, Positive]]  \n",
       "4  [[22, 31, Negative], [36, 57, Negative]]  "
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# absa needs to be read properly from the GRACE formatting so that it can be converted to a df\n",
    "file_path = \"GRACE/data/laptops_2014_all.txt\"\n",
    "df_lap14 = grace_format_to_df_cols(file_path)\n",
    "\n",
    "print(len(df_lap14))\n",
    "df_lap14.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocess the data to fit the GRACE Model format\n",
    "\n",
    "# # text column to list\n",
    "# text_list = list(df_lap14['text'].astype(str))\n",
    "\n",
    "# # input format for GRACE model\n",
    "# addition = ' - - O O O'\n",
    "# convert_to_doc = []\n",
    "\n",
    "# # iteratively apply re-formatting and save to new list\n",
    "# for tweet in text_list:\n",
    "#     words = tweet.split()\n",
    "#     words_with_addition = []\n",
    "#     for word in words:\n",
    "#         new_word = word + addition\n",
    "#         #print(new_word)\n",
    "#         words_with_addition.append(new_word)\n",
    "#     convert_to_doc.append(words_with_addition)\n",
    "\n",
    "# # Save in the Case study folder\n",
    "# path_to_reformatted_data = \"GRACE/data/laptops_2014_all_reformatted.txt\"\n",
    "# with open(path_to_reformatted_data, mode = \"w\", encoding='utf-8') as f:\n",
    "#     f.write(\"-DOCSTART-\\n\\n\")\n",
    "#     for tweet in convert_to_doc:\n",
    "#         for word in tweet:\n",
    "#             f.write(\"%s\\n\" % word)\n",
    "#         f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded val file: GRACE/data/laptops_2014_all_reformatted.txt\n"
     ]
    }
   ],
   "source": [
    "path_to_reformatted_data = \"GRACE/data/laptops_2014_all_reformatted.txt\"\n",
    "\n",
    "DATALOADER_DICT = {}\n",
    "# only \"eval\" state is needed (\"train\" is left out)\n",
    "DATALOADER_DICT[\"ate_asc\"] = {\"eval\":dataloader_val}\n",
    "# Create dataloader with the prepared dataset\n",
    "eval_dataloader, eval_examples = DATALOADER_DICT[task_name][\"eval\"](args, tokenizer, path_to_reformatted_data, label_tp_list=label_tp_list, set_type=\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Model on Prepared Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 121/121 [00:17<00:00,  6.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# empty lists for both the identified aspect terms and the related emotions\n",
    "pred_aspect_terms = []\n",
    "pred_aspect_emotions = []\n",
    "\n",
    "# for-loop to iterate over the preprocessed outputs from the \"eval_dataloader\" \n",
    "# for input_ids, input_mask, segment_ids, at_label_ids, as_label_ids, label_mask, label_mask_X in eval_dataloader:\n",
    "for input_ids, input_mask, segment_ids, at_label_ids, as_label_ids, label_mask, label_mask_X in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    at_label_ids = at_label_ids.to(device)\n",
    "    as_label_ids = as_label_ids.to(device)\n",
    "    label_mask = label_mask.to(device)\n",
    "    label_mask_X = label_mask_X.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # logits, decoder_logits = model(input_ids, segment_ids, input_mask)\n",
    "        logits, sequence_output, encoder_output = model.get_encoder_logits(input_ids, segment_ids, input_mask)\n",
    "        pred_dec_ids = torch.argmax(F.log_softmax(logits, dim=2), dim=2)\n",
    "        decoder_logits = model.get_decoder_logits(encoder_output, input_mask, label_mask_X, pred_dec_ids)\n",
    "        logits = torch.argmax(F.log_softmax(logits, dim=2), dim=2)\n",
    "        decoder_logits = torch.argmax(F.log_softmax(decoder_logits, dim=2), dim=2)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        decoder_logits = decoder_logits.detach().cpu().numpy()\n",
    "        \n",
    "    at_label_ids = at_label_ids.to('cpu').numpy()\n",
    "    as_label_ids = as_label_ids.to('cpu').numpy()\n",
    "    label_mask = label_mask.to('cpu').numpy()\n",
    "    \n",
    "    for i, mask_i in enumerate(label_mask):\n",
    "        temp_1 = []\n",
    "        temp_2 = []\n",
    "        for j, l in enumerate(mask_i):\n",
    "            if l > -1:\n",
    "                temp_1.append(at_label_map[logits[i][j]])\n",
    "                temp_2.append(as_label_map[decoder_logits[i][j]])\n",
    "                \n",
    "        pred_aspect_terms.append(temp_1)\n",
    "        pred_aspect_emotions.append(temp_2)\n",
    "\n",
    "# add new aspect term labels and aspect sentiment labels as columns to twemlab dataframe\n",
    "df_lap14['aspect_term_preds'] = pred_aspect_terms\n",
    "df_lap14['aspect_emo_preds'] = pred_aspect_emotions\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Next step, extract the aspect term and sentiment and store in twemlab dataframe\n",
    "aspect_terms = []\n",
    "aspect_sentiments = []\n",
    "\n",
    "# for every row (tweet)\n",
    "for idx, row in df_lap14.iterrows():\n",
    "    \n",
    "    row_aspect_terms = []\n",
    "    row_aspect_sentiments = []\n",
    "\n",
    "    # split text into individual words\n",
    "    words = row['text'].split()\n",
    "\n",
    "    # Loop through aspect_term_preds\n",
    "    for i, pred in enumerate(row['aspect_term_preds']):\n",
    "        if pred == 'B-AP':\n",
    "            # Initialize term with the current word\n",
    "            term = words[i] if i < len(words) else ''\n",
    "            sent = row['aspect_emo_preds'][i]\n",
    "\n",
    "            # check for continuation of aspect term ('I-AP')\n",
    "            j = i + 1\n",
    "            while j < len(row['aspect_term_preds']) and row['aspect_term_preds'][j] == 'I-AP':\n",
    "                # Append word to term if within bounds\n",
    "                if j < len(words):\n",
    "                    term += ' ' + words[j]\n",
    "                    sent += ' ' + row['aspect_emo_preds'][j]\n",
    "                j += 1\n",
    "\n",
    "            # save the aspect term and emotion\n",
    "            if term:  # Only add non-empty terms\n",
    "                row_aspect_terms.append(term)\n",
    "                row_aspect_sentiments.append(sent)\n",
    "\n",
    "    # append extracted terms and sentiments to respective lists\n",
    "    aspect_terms.append(row_aspect_terms)\n",
    "    aspect_sentiments.append(row_aspect_sentiments)\n",
    "\n",
    "# assign the lists back to the DataFrame\n",
    "df_lap14['aspect_terms'] = aspect_terms\n",
    "df_lap14['aspect_emotions'] = aspect_sentiments\n",
    "\n",
    "# remove the previous columns\n",
    "df_lap14.drop(columns=['aspect_term_preds', 'aspect_emo_preds'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>aspect_terms</th>\n",
       "      <th>aspect_emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3166</th>\n",
       "      <td>I love the solid machined aluminum frame , and...</td>\n",
       "      <td>[[17, 40, Positive], [51, 59, Positive]]</td>\n",
       "      <td>[aluminum frame, keyboard, laptop]</td>\n",
       "      <td>[NONE NONE, NONE, NONE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3400</th>\n",
       "      <td>I bought this eMachines Notebook PC in January...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[eMachines, Notebook PC, it]</td>\n",
       "      <td>[FEAR, FEAR FEAR, FEAR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>No installation disk -LRB- DVD -RRB- is includ...</td>\n",
       "      <td>[[3, 36, Neutral]]</td>\n",
       "      <td>[installation disk]</td>\n",
       "      <td>[FEAR FEAR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>Thanks for the fast shipment and great price .</td>\n",
       "      <td>[[20, 28, Positive], [39, 44, Positive]]</td>\n",
       "      <td>[shipment, price]</td>\n",
       "      <td>[NONE, NONE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1620</th>\n",
       "      <td>The computer runs very fast with no problems a...</td>\n",
       "      <td>[[13, 17, Positive], [53, 67, Positive], [93, ...</td>\n",
       "      <td>[computer, iLife software, iMovie, GarageBand]</td>\n",
       "      <td>[NONE, NONE NONE, NONE, NONE]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "3166  I love the solid machined aluminum frame , and...   \n",
       "3400  I bought this eMachines Notebook PC in January...   \n",
       "8     No installation disk -LRB- DVD -RRB- is includ...   \n",
       "345      Thanks for the fast shipment and great price .   \n",
       "1620  The computer runs very fast with no problems a...   \n",
       "\n",
       "                                                  label  \\\n",
       "3166           [[17, 40, Positive], [51, 59, Positive]]   \n",
       "3400                                                 []   \n",
       "8                                    [[3, 36, Neutral]]   \n",
       "345            [[20, 28, Positive], [39, 44, Positive]]   \n",
       "1620  [[13, 17, Positive], [53, 67, Positive], [93, ...   \n",
       "\n",
       "                                        aspect_terms  \\\n",
       "3166              [aluminum frame, keyboard, laptop]   \n",
       "3400                    [eMachines, Notebook PC, it]   \n",
       "8                                [installation disk]   \n",
       "345                                [shipment, price]   \n",
       "1620  [computer, iLife software, iMovie, GarageBand]   \n",
       "\n",
       "                    aspect_emotions  \n",
       "3166        [NONE NONE, NONE, NONE]  \n",
       "3400        [FEAR, FEAR FEAR, FEAR]  \n",
       "8                       [FEAR FEAR]  \n",
       "345                    [NONE, NONE]  \n",
       "1620  [NONE, NONE NONE, NONE, NONE]  "
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lap14.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Precision, Recall, f1 on the Laptops 2014 Dataset, on which the trained GRACE ABEA model has run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 1810\n",
      "FP: 5720\n",
      "FN 1143\n",
      "Partial Matches: 2188\n",
      "{'Precision': 0.2403718459495352, 'Recall': 0.6129359972908907, 'F1 Score': 0.34532099589812076, 'Partial Matches': 2188}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def extract_terms(text, indices):\n",
    "    terms = []\n",
    "    for index in indices:\n",
    "        start, end = index[0], index[1]\n",
    "        terms.append(text[start:end].strip())\n",
    "    return terms\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    partial_matches = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        labels = row['label']\n",
    "        ground_truth_terms = set([extract_terms(text, [label[:2]])[0] for label in labels if label]) if labels else set()\n",
    "        \n",
    "        predicted_terms = set(row['aspect_terms'])\n",
    "\n",
    "        # Count True Positives (exact match)\n",
    "        TP += len(ground_truth_terms.intersection(predicted_terms))\n",
    "        \n",
    "        # Count False Positives\n",
    "        FP += len(predicted_terms - ground_truth_terms)\n",
    "        \n",
    "        # Count False Negatives\n",
    "        FN += len(ground_truth_terms - predicted_terms)\n",
    "\n",
    "        # Optionally handle partial matches\n",
    "        for pred_term in predicted_terms:\n",
    "            for gt_term in ground_truth_terms:\n",
    "                if pred_term in gt_term or gt_term in pred_term:\n",
    "                    partial_matches += 1\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"TP: {TP}\")\n",
    "    print(f\"FP: {FP}\")\n",
    "    print(f\"FN {FN}\")\n",
    "    print(f\"Partial Matches: {partial_matches}\")\n",
    "    return {\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Partial Matches': partial_matches\n",
    "    }\n",
    "\n",
    "metrics = calculate_metrics(df_lap14)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Precision': 0.3265, 'Recall': 0.6879, 'F1 Score': 0.4428}\n"
     ]
    }
   ],
   "source": [
    "def extract_words(text, indices):\n",
    "    words = []\n",
    "    for index in indices:\n",
    "        start, end = index[0], index[1]\n",
    "        # Split the extracted term into words and extend the list\n",
    "        words.extend(text[start:end].strip().split())\n",
    "    return words\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        labels = row['label']\n",
    "        # Extract all words from the ground truth terms\n",
    "        ground_truth_words = set(extract_words(text, [label[:2] for label in labels if label])) if labels else set()\n",
    "        # print(f\"Labels: {ground_truth_words}\")\n",
    "\n",
    "        # Extract all words from the predicted terms\n",
    "        predicted_words = set()\n",
    "        for term in row['aspect_terms']:\n",
    "            predicted_words.update(term.split())\n",
    "        # print(f\"Predictions: {predicted_words}\")\n",
    "\n",
    "        # Count True Positives (exact word match)\n",
    "        if not ground_truth_words and not predicted_words:\n",
    "            TP += 1  # Correctly predicted no terms\n",
    "            # print(\"TP: no aspect terms\")\n",
    "        else:\n",
    "            TP += len(ground_truth_words.intersection(predicted_words))\n",
    "            # print(f\"TP: {ground_truth_words.intersection(predicted_words)}\")\n",
    "\n",
    "        # Count False Positives\n",
    "        FP += len(predicted_words - ground_truth_words)\n",
    "        # print(f\"FP: {predicted_words - ground_truth_words}\")\n",
    "\n",
    "        # Count False Negatives\n",
    "        FN += len(ground_truth_words - predicted_words)\n",
    "        # print(f\"FN: {ground_truth_words - predicted_words}\")\n",
    "\n",
    "        # print(\"-------\")\n",
    "        \n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'Precision': round(precision, 4),\n",
    "        'Recall':  round(recall, 4),\n",
    "        'F1 Score': round(f1, 4)\n",
    "    }\n",
    "\n",
    "# Assuming 'df' is already defined and contains the columns 'text', 'label', and 'aspect_terms'\n",
    "metrics_lap14 = calculate_metrics(df_lap14)\n",
    "print(metrics_lap14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATE Validation: Restaurants Union SemEval Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read the GRACE-formatted txt file into a df with a 'text' and 'label' column.\n",
    "def grace_format_to_df_cols(txt_path):\n",
    "\n",
    "    with open(txt_path, 'r') as file:\n",
    "        file_content = file.read()\n",
    "    \n",
    "    content_without_docstart = file_content.split('\\n', 1)[1]\n",
    "\n",
    "    # Split the content based on empty lines to get each sequence\n",
    "    sequences = [seq.strip() for seq in content_without_docstart.split('\\n\\n') if seq.strip()]\n",
    "\n",
    "    texts_labels = [process_sequence(seq) for seq in sequences]\n",
    "    texts, labels = zip(*texts_labels)  # Unzip the texts and labels\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'text': texts,\n",
    "        'label': labels\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "# function to process each of the sequences\n",
    "def process_sequence(seq):\n",
    "    words = []\n",
    "    labels = []\n",
    "    char_index = 0  # Start from the first character\n",
    "    in_label = False\n",
    "    label_start = 0\n",
    "    label_sentiment = ''\n",
    "\n",
    "    for line in seq.split('\\n'):\n",
    "        parts = line.split()\n",
    "        if len(parts) < 2:  # Skip empty lines or lines without tags\n",
    "            continue\n",
    "        word, tag = parts[0], parts[-1]\n",
    "        words.append(word)\n",
    "\n",
    "        # If this word starts a new label\n",
    "        if 'B_AP' in tag:\n",
    "            if in_label:  # End the previous label if starting a new one\n",
    "                labels.append([label_start, char_index - 1, label_sentiment])\n",
    "            in_label = True\n",
    "            label_start = char_index\n",
    "            # Determine the sentiment\n",
    "            if 'NEUTRAL' in tag:\n",
    "                label_sentiment = 'Neutral'\n",
    "            elif 'POSITIVE' in tag:\n",
    "                label_sentiment = 'Positive'\n",
    "            elif 'NEGATIVE' in tag:\n",
    "                label_sentiment = 'Negative'\n",
    "                \n",
    "        # If this word is not part of a label or starts a new label\n",
    "        if 'B_AP' not in tag and 'I_AP' not in tag and in_label:\n",
    "            labels.append([label_start, char_index - 1, label_sentiment])\n",
    "            in_label = False\n",
    "\n",
    "        char_index += len(word) + 1  # Update character index for next word, adding 1 for the space\n",
    "\n",
    "    # If the last label goes till the end of the sequence\n",
    "    if in_label:\n",
    "        labels.append([label_start, char_index - 1, label_sentiment])\n",
    "\n",
    "    return ' '.join(words), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7286\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The bread is top notch as well .</td>\n",
       "      <td>[[4, 9, Positive]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have to say they have one of the fastest del...</td>\n",
       "      <td>[[43, 57, Positive]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Food is always fresh and hot - ready to eat !</td>\n",
       "      <td>[[0, 4, Positive]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Did I mention that the coffee is OUTSTANDING ?</td>\n",
       "      <td>[[23, 29, Positive]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Certainly not the best sushi in New York , how...</td>\n",
       "      <td>[[23, 28, ], [82, 87, Positive]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                   The bread is top notch as well .   \n",
       "1  I have to say they have one of the fastest del...   \n",
       "2      Food is always fresh and hot - ready to eat !   \n",
       "3     Did I mention that the coffee is OUTSTANDING ?   \n",
       "4  Certainly not the best sushi in New York , how...   \n",
       "\n",
       "                              label  \n",
       "0                [[4, 9, Positive]]  \n",
       "1              [[43, 57, Positive]]  \n",
       "2                [[0, 4, Positive]]  \n",
       "3              [[23, 29, Positive]]  \n",
       "4  [[23, 28, ], [82, 87, Positive]]  "
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# absa needs to be read properly from the GRACE formatting so that it can be converted to a df\n",
    "file_path = \"GRACE/data/restaurants_union_all.txt\"\n",
    "df_rest_all = grace_format_to_df_cols(file_path)\n",
    "\n",
    "print(len(df_rest_all))\n",
    "df_rest_all.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocess the data to fit the GRACE Model format\n",
    "\n",
    "# # text column to list\n",
    "# text_list = list(df_rest_all['text'].astype(str))\n",
    "\n",
    "# # input format for GRACE model\n",
    "# addition = ' - - O O O'\n",
    "# convert_to_doc = []\n",
    "\n",
    "# # iteratively apply re-formatting and save to new list\n",
    "# for tweet in text_list:\n",
    "#     words = tweet.split()\n",
    "#     words_with_addition = []\n",
    "#     for word in words:\n",
    "#         new_word = word + addition\n",
    "#         #print(new_word)\n",
    "#         words_with_addition.append(new_word)\n",
    "#     convert_to_doc.append(words_with_addition)\n",
    "\n",
    "# # Save in the Case study folder\n",
    "# path_to_reformatted_data = \"GRACE/data/restaurants_union_all_reformatted.txt\"\n",
    "# with open(path_to_reformatted_data, mode = \"w\", encoding='utf-8') as f:\n",
    "#     f.write(\"-DOCSTART-\\n\\n\")\n",
    "#     for tweet in convert_to_doc:\n",
    "#         for word in tweet:\n",
    "#             f.write(\"%s\\n\" % word)\n",
    "#         f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded val file: GRACE/data/restaurants_union_all_reformatted.txt\n"
     ]
    }
   ],
   "source": [
    "path_to_reformatted_data = \"GRACE/data/restaurants_union_all_reformatted.txt\"\n",
    "\n",
    "\n",
    "DATALOADER_DICT = {}\n",
    "# only \"eval\" state is needed (\"train\" is left out)\n",
    "DATALOADER_DICT[\"ate_asc\"] = {\"eval\":dataloader_val}\n",
    "# Create dataloader with the prepared dataset\n",
    "eval_dataloader, eval_examples = DATALOADER_DICT[task_name][\"eval\"](args, tokenizer, path_to_reformatted_data, label_tp_list=label_tp_list, set_type=\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Model on Prepared Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 228/228 [00:34<00:00,  6.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# empty lists for both the identified aspect terms and the related emotions\n",
    "pred_aspect_terms = []\n",
    "pred_aspect_emotions = []\n",
    "\n",
    "# for-loop to iterate over the preprocessed outputs from the \"eval_dataloader\" \n",
    "# for input_ids, input_mask, segment_ids, at_label_ids, as_label_ids, label_mask, label_mask_X in eval_dataloader:\n",
    "for input_ids, input_mask, segment_ids, at_label_ids, as_label_ids, label_mask, label_mask_X in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    at_label_ids = at_label_ids.to(device)\n",
    "    as_label_ids = as_label_ids.to(device)\n",
    "    label_mask = label_mask.to(device)\n",
    "    label_mask_X = label_mask_X.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # logits, decoder_logits = model(input_ids, segment_ids, input_mask)\n",
    "        logits, sequence_output, encoder_output = model.get_encoder_logits(input_ids, segment_ids, input_mask)\n",
    "        pred_dec_ids = torch.argmax(F.log_softmax(logits, dim=2), dim=2)\n",
    "        decoder_logits = model.get_decoder_logits(encoder_output, input_mask, label_mask_X, pred_dec_ids)\n",
    "        logits = torch.argmax(F.log_softmax(logits, dim=2), dim=2)\n",
    "        decoder_logits = torch.argmax(F.log_softmax(decoder_logits, dim=2), dim=2)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        decoder_logits = decoder_logits.detach().cpu().numpy()\n",
    "        \n",
    "    at_label_ids = at_label_ids.to('cpu').numpy()\n",
    "    as_label_ids = as_label_ids.to('cpu').numpy()\n",
    "    label_mask = label_mask.to('cpu').numpy()\n",
    "    \n",
    "    for i, mask_i in enumerate(label_mask):\n",
    "        temp_1 = []\n",
    "        temp_2 = []\n",
    "        for j, l in enumerate(mask_i):\n",
    "            if l > -1:\n",
    "                temp_1.append(at_label_map[logits[i][j]])\n",
    "                temp_2.append(as_label_map[decoder_logits[i][j]])\n",
    "                \n",
    "        pred_aspect_terms.append(temp_1)\n",
    "        pred_aspect_emotions.append(temp_2)\n",
    "\n",
    "# add new aspect term labels and aspect sentiment labels as columns to twemlab dataframe\n",
    "df_rest_all['aspect_term_preds'] = pred_aspect_terms\n",
    "df_rest_all['aspect_emo_preds'] = pred_aspect_emotions\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Next step, extract the aspect term and sentiment and store in twemlab dataframe\n",
    "aspect_terms = []\n",
    "aspect_sentiments = []\n",
    "\n",
    "# for every row (tweet)\n",
    "for idx, row in df_rest_all.iterrows():\n",
    "    \n",
    "    row_aspect_terms = []\n",
    "    row_aspect_sentiments = []\n",
    "\n",
    "    # split text into individual words\n",
    "    words = row['text'].split()\n",
    "\n",
    "    # Loop through aspect_term_preds\n",
    "    for i, pred in enumerate(row['aspect_term_preds']):\n",
    "        if pred == 'B-AP':\n",
    "            # Initialize term with the current word\n",
    "            term = words[i] if i < len(words) else ''\n",
    "            sent = row['aspect_emo_preds'][i]\n",
    "\n",
    "            # check for continuation of aspect term ('I-AP')\n",
    "            j = i + 1\n",
    "            while j < len(row['aspect_term_preds']) and row['aspect_term_preds'][j] == 'I-AP':\n",
    "                # Append word to term if within bounds\n",
    "                if j < len(words):\n",
    "                    term += ' ' + words[j]\n",
    "                    sent += ' ' + row['aspect_emo_preds'][j]\n",
    "                j += 1\n",
    "\n",
    "            # save the aspect term and emotion\n",
    "            if term:  # Only add non-empty terms\n",
    "                row_aspect_terms.append(term)\n",
    "                row_aspect_sentiments.append(sent)\n",
    "\n",
    "    # append extracted terms and sentiments to respective lists\n",
    "    aspect_terms.append(row_aspect_terms)\n",
    "    aspect_sentiments.append(row_aspect_sentiments)\n",
    "\n",
    "# assign the lists back to the DataFrame\n",
    "df_rest_all['aspect_terms'] = aspect_terms\n",
    "df_rest_all['aspect_emotions'] = aspect_sentiments\n",
    "\n",
    "# remove the previous columns\n",
    "df_rest_all.drop(columns=['aspect_term_preds', 'aspect_emo_preds'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>aspect_terms</th>\n",
       "      <th>aspect_emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1688</th>\n",
       "      <td>The band was very good and the service was att...</td>\n",
       "      <td>[[4, 8, Positive], [31, 38, Positive]]</td>\n",
       "      <td>[band, service]</td>\n",
       "      <td>[NONE, NONE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7253</th>\n",
       "      <td>For dinner I had the chicken tikka-masala and ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[chicken tikka-masala, garlic naan]</td>\n",
       "      <td>[NONE NONE, NONE NONE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5331</th>\n",
       "      <td>Over 100 different choices to create your own .</td>\n",
       "      <td>[]</td>\n",
       "      <td>[choices]</td>\n",
       "      <td>[NONE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>The sommelier is fantastic , down-to-earth , &amp;...</td>\n",
       "      <td>[[4, 13, Positive]]</td>\n",
       "      <td>[sommelier]</td>\n",
       "      <td>[NONE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>The food was good overall .</td>\n",
       "      <td>[[4, 8, Positive]]</td>\n",
       "      <td>[food]</td>\n",
       "      <td>[NONE]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "1688  The band was very good and the service was att...   \n",
       "7253  For dinner I had the chicken tikka-masala and ...   \n",
       "5331    Over 100 different choices to create your own .   \n",
       "1697  The sommelier is fantastic , down-to-earth , &...   \n",
       "649                         The food was good overall .   \n",
       "\n",
       "                                       label  \\\n",
       "1688  [[4, 8, Positive], [31, 38, Positive]]   \n",
       "7253                                      []   \n",
       "5331                                      []   \n",
       "1697                     [[4, 13, Positive]]   \n",
       "649                       [[4, 8, Positive]]   \n",
       "\n",
       "                             aspect_terms         aspect_emotions  \n",
       "1688                      [band, service]            [NONE, NONE]  \n",
       "7253  [chicken tikka-masala, garlic naan]  [NONE NONE, NONE NONE]  \n",
       "5331                            [choices]                  [NONE]  \n",
       "1697                          [sommelier]                  [NONE]  \n",
       "649                                [food]                  [NONE]  "
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rest_all.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 6151\n",
      "FP: 8620\n",
      "FN 1517\n",
      "Partial Matches: 7238\n",
      "{'Precision': 0.4164240741994449, 'Recall': 0.8021648408972353, 'F1 Score': 0.5482419002629351, 'Partial Matches': 7238}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def extract_terms(text, indices):\n",
    "    terms = []\n",
    "    for index in indices:\n",
    "        start, end = index[0], index[1]\n",
    "        terms.append(text[start:end].strip())\n",
    "    return terms\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    partial_matches = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        labels = row['label']\n",
    "        ground_truth_terms = set([extract_terms(text, [label[:2]])[0] for label in labels if label]) if labels else set()\n",
    "        \n",
    "        predicted_terms = set(row['aspect_terms'])\n",
    "\n",
    "        # Count True Positives (exact match)\n",
    "        TP += len(ground_truth_terms.intersection(predicted_terms))\n",
    "        \n",
    "        # Count False Positives\n",
    "        FP += len(predicted_terms - ground_truth_terms)\n",
    "        \n",
    "        # Count False Negatives\n",
    "        FN += len(ground_truth_terms - predicted_terms)\n",
    "\n",
    "        # Optionally handle partial matches\n",
    "        for pred_term in predicted_terms:\n",
    "            for gt_term in ground_truth_terms:\n",
    "                if pred_term in gt_term or gt_term in pred_term:\n",
    "                    partial_matches += 1\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"TP: {TP}\")\n",
    "    print(f\"FP: {FP}\")\n",
    "    print(f\"FN {FN}\")\n",
    "    print(f\"Partial Matches: {partial_matches}\")\n",
    "    return {\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Partial Matches': partial_matches\n",
    "    }\n",
    "\n",
    "metrics = calculate_metrics(df_rest_all)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Precision': 0.5, 'Recall': 0.8338, 'F1 Score': 0.6251}\n"
     ]
    }
   ],
   "source": [
    "def extract_words(text, indices):\n",
    "    words = []\n",
    "    for index in indices:\n",
    "        start, end = index[0], index[1]\n",
    "        # Split the extracted term into words and extend the list\n",
    "        words.extend(text[start:end].strip().split())\n",
    "    return words\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        labels = row['label']\n",
    "        # Extract all words from the ground truth terms\n",
    "        ground_truth_words = set(extract_words(text, [label[:2] for label in labels if label])) if labels else set()\n",
    "        # print(f\"Labels: {ground_truth_words}\")\n",
    "\n",
    "        # Extract all words from the predicted terms\n",
    "        predicted_words = set()\n",
    "        for term in row['aspect_terms']:\n",
    "            predicted_words.update(term.split())\n",
    "        # print(f\"Predictions: {predicted_words}\")\n",
    "\n",
    "        # Count True Positives (exact word match)\n",
    "        if not ground_truth_words and not predicted_words:\n",
    "            TP += 1  # Correctly predicted no terms\n",
    "            # print(\"TP: no aspect terms\")\n",
    "        else:\n",
    "            TP += len(ground_truth_words.intersection(predicted_words))\n",
    "            # print(f\"TP: {ground_truth_words.intersection(predicted_words)}\")\n",
    "\n",
    "        # Count False Positives\n",
    "        FP += len(predicted_words - ground_truth_words)\n",
    "        # print(f\"FP: {predicted_words - ground_truth_words}\")\n",
    "\n",
    "        # Count False Negatives\n",
    "        FN += len(ground_truth_words - predicted_words)\n",
    "        # print(f\"FN: {ground_truth_words - predicted_words}\")\n",
    "\n",
    "        # print(\"-------\")\n",
    "        \n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'Precision': round(precision, 4),\n",
    "        'Recall':  round(recall, 4),\n",
    "        'F1 Score': round(f1, 4)\n",
    "    }\n",
    "\n",
    "# Assuming 'df' is already defined and contains the columns 'text', 'label', and 'aspect_terms'\n",
    "metrics_rest_all = calculate_metrics(df_rest_all)\n",
    "print(metrics_rest_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion Validation: SemEval2018 Affect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12634\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "df_affect = pd.read_csv(\"GRACE/data/SemEval Affect EI-reg/EI-reg-En-all.tsv\", sep=\"\\t\")\n",
    "print(len(df_affect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Affect Dimension</th>\n",
       "      <th>Intensity Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-En-01346</td>\n",
       "      <td>Life is too short to be jealous, hating, keepi...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-En-03526</td>\n",
       "      <td>@stephdavis77 Your parents should get you help...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-En-30164</td>\n",
       "      <td>@jimadair3 Guitar shop owners everywhere rejoice</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-En-41513</td>\n",
       "      <td>Sometimes the support network is causing the d...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-En-00995</td>\n",
       "      <td>'....trying to work out how a band featuring C...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                              Tweet  \\\n",
       "0  2018-En-01346  Life is too short to be jealous, hating, keepi...   \n",
       "1  2018-En-03526  @stephdavis77 Your parents should get you help...   \n",
       "2  2017-En-30164   @jimadair3 Guitar shop owners everywhere rejoice   \n",
       "3  2017-En-41513  Sometimes the support network is causing the d...   \n",
       "4  2018-En-00995  '....trying to work out how a band featuring C...   \n",
       "\n",
       "  Affect Dimension  Intensity Score  \n",
       "0             fear            0.500  \n",
       "1             fear            0.854  \n",
       "2              joy            0.667  \n",
       "3          sadness            0.854  \n",
       "4          sadness            0.739  "
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_affect = df_affect.sample(frac=1).reset_index(drop=True)\n",
    "df_affect.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocess the data to fit the GRACE Model format\n",
    "\n",
    "# # text column to list\n",
    "# text_list = list(df_affect['Tweet'].astype(str))\n",
    "\n",
    "# # input format for GRACE model\n",
    "# addition = ' - - O O O'\n",
    "# convert_to_doc = []\n",
    "\n",
    "# # iteratively apply re-formatting and save to new list\n",
    "# for tweet in text_list:\n",
    "#     words = tweet.split()\n",
    "#     words_with_addition = []\n",
    "#     for word in words:\n",
    "#         new_word = word + addition\n",
    "#         #print(new_word)\n",
    "#         words_with_addition.append(new_word)\n",
    "#     convert_to_doc.append(words_with_addition)\n",
    "\n",
    "# # Save in the Case study folder\n",
    "# path_to_reformatted_data = \"GRACE/data/semeval_affect_reformatted.txt\"\n",
    "# with open(path_to_reformatted_data, mode = \"w\", encoding='utf-8') as f:\n",
    "#     f.write(\"-DOCSTART-\\n\\n\")\n",
    "#     for tweet in convert_to_doc:\n",
    "#         for word in tweet:\n",
    "#             f.write(\"%s\\n\" % word)\n",
    "#         f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded val file: GRACE/data/semeval_affect_reformatted.txt\n"
     ]
    }
   ],
   "source": [
    "path_to_reformatted_data = \"GRACE/data/semeval_affect_reformatted.txt\"\n",
    "\n",
    "DATALOADER_DICT = {}\n",
    "# only \"eval\" state is needed (\"train\" is left out)\n",
    "DATALOADER_DICT[\"ate_asc\"] = {\"eval\":dataloader_val}\n",
    "# Create dataloader with the prepared dataset\n",
    "eval_dataloader, eval_examples = DATALOADER_DICT[task_name][\"eval\"](args, tokenizer, path_to_reformatted_data, label_tp_list=label_tp_list, set_type=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [00:58<00:00,  6.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# empty lists for both the identified aspect terms and the related emotions\n",
    "pred_aspect_terms = []\n",
    "pred_aspect_emotions = []\n",
    "\n",
    "# for-loop to iterate over the preprocessed outputs from the \"eval_dataloader\" \n",
    "# for input_ids, input_mask, segment_ids, at_label_ids, as_label_ids, label_mask, label_mask_X in eval_dataloader:\n",
    "for input_ids, input_mask, segment_ids, at_label_ids, as_label_ids, label_mask, label_mask_X in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    at_label_ids = at_label_ids.to(device)\n",
    "    as_label_ids = as_label_ids.to(device)\n",
    "    label_mask = label_mask.to(device)\n",
    "    label_mask_X = label_mask_X.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # logits, decoder_logits = model(input_ids, segment_ids, input_mask)\n",
    "        logits, sequence_output, encoder_output = model.get_encoder_logits(input_ids, segment_ids, input_mask)\n",
    "        pred_dec_ids = torch.argmax(F.log_softmax(logits, dim=2), dim=2)\n",
    "        decoder_logits = model.get_decoder_logits(encoder_output, input_mask, label_mask_X, pred_dec_ids)\n",
    "        logits = torch.argmax(F.log_softmax(logits, dim=2), dim=2)\n",
    "        decoder_logits = torch.argmax(F.log_softmax(decoder_logits, dim=2), dim=2)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        decoder_logits = decoder_logits.detach().cpu().numpy()\n",
    "        \n",
    "    at_label_ids = at_label_ids.to('cpu').numpy()\n",
    "    as_label_ids = as_label_ids.to('cpu').numpy()\n",
    "    label_mask = label_mask.to('cpu').numpy()\n",
    "    \n",
    "    for i, mask_i in enumerate(label_mask):\n",
    "        temp_1 = []\n",
    "        temp_2 = []\n",
    "        for j, l in enumerate(mask_i):\n",
    "            if l > -1:\n",
    "                temp_1.append(at_label_map[logits[i][j]])\n",
    "                temp_2.append(as_label_map[decoder_logits[i][j]])\n",
    "                \n",
    "        pred_aspect_terms.append(temp_1)\n",
    "        pred_aspect_emotions.append(temp_2)\n",
    "\n",
    "# add new aspect term labels and aspect sentiment labels as columns to twemlab dataframe\n",
    "df_affect['aspect_term_preds'] = pred_aspect_terms\n",
    "df_affect['aspect_emo_preds'] = pred_aspect_emotions\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Next step, extract the aspect term and sentiment and store in twemlab dataframe\n",
    "aspect_terms = []\n",
    "aspect_sentiments = []\n",
    "\n",
    "# for every row (tweet)\n",
    "for idx, row in df_affect.iterrows():\n",
    "    \n",
    "    row_aspect_terms = []\n",
    "    row_aspect_sentiments = []\n",
    "\n",
    "    # split text into individual words\n",
    "    words = row['Tweet'].split()\n",
    "\n",
    "    # Loop through aspect_term_preds\n",
    "    for i, pred in enumerate(row['aspect_term_preds']):\n",
    "        if pred == 'B-AP':\n",
    "            # Initialize term with the current word\n",
    "            term = words[i] if i < len(words) else ''\n",
    "            sent = row['aspect_emo_preds'][i]\n",
    "\n",
    "            # check for continuation of aspect term ('I-AP')\n",
    "            j = i + 1\n",
    "            while j < len(row['aspect_term_preds']) and row['aspect_term_preds'][j] == 'I-AP':\n",
    "                # Append word to term if within bounds\n",
    "                if j < len(words):\n",
    "                    term += ' ' + words[j]\n",
    "                    sent += ' ' + row['aspect_emo_preds'][j]\n",
    "                j += 1\n",
    "\n",
    "            # save the aspect term and emotion\n",
    "            if term:  # Only add non-empty terms\n",
    "                row_aspect_terms.append(term)\n",
    "                row_aspect_sentiments.append(sent)\n",
    "\n",
    "    # append extracted terms and sentiments to respective lists\n",
    "    aspect_terms.append(row_aspect_terms)\n",
    "    aspect_sentiments.append(row_aspect_sentiments)\n",
    "\n",
    "# assign the lists back to the DataFrame\n",
    "df_affect['aspect_terms'] = aspect_terms\n",
    "df_affect['aspect_emotions'] = aspect_sentiments\n",
    "\n",
    "# remove the previous columns\n",
    "df_affect.drop(columns=['aspect_term_preds', 'aspect_emo_preds'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Affect Dimension</th>\n",
       "      <th>Intensity Score</th>\n",
       "      <th>aspect_terms</th>\n",
       "      <th>aspect_emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10998</th>\n",
       "      <td>2018-En-02023</td>\n",
       "      <td>It's a #disappointing #frightening #sickening ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.839</td>\n",
       "      <td>[#fail... #parents]</td>\n",
       "      <td>[NONE NONE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3854</th>\n",
       "      <td>2017-En-20431</td>\n",
       "      <td>@NeyaphemMaster @_James_Kellar_ @RavenMetamorp...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.562</td>\n",
       "      <td>[@MagnetoBroHood, Thomas', group's focus]</td>\n",
       "      <td>[NONE, NONE, NONE NONE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4493</th>\n",
       "      <td>2017-En-22235</td>\n",
       "      <td>don't worry mary berry, my banter is as dry as...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.333</td>\n",
       "      <td>[worry, as, x]</td>\n",
       "      <td>[NONE, NONE, NONE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7087</th>\n",
       "      <td>2017-En-11638</td>\n",
       "      <td>Just love Matthew Parris! Political rage is good!</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.372</td>\n",
       "      <td>[love, rage]</td>\n",
       "      <td>[NONE, NONE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3010</th>\n",
       "      <td>2018-En-03496</td>\n",
       "      <td>The best bridge between despair and hope is a ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.571</td>\n",
       "      <td>[and]</td>\n",
       "      <td>[NONE]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ID                                              Tweet  \\\n",
       "10998  2018-En-02023  It's a #disappointing #frightening #sickening ...   \n",
       "3854   2017-En-20431  @NeyaphemMaster @_James_Kellar_ @RavenMetamorp...   \n",
       "4493   2017-En-22235  don't worry mary berry, my banter is as dry as...   \n",
       "7087   2017-En-11638  Just love Matthew Parris! Political rage is good!   \n",
       "3010   2018-En-03496  The best bridge between despair and hope is a ...   \n",
       "\n",
       "      Affect Dimension  Intensity Score  \\\n",
       "10998             fear            0.839   \n",
       "3854              fear            0.562   \n",
       "4493              fear            0.333   \n",
       "7087             anger            0.372   \n",
       "3010           sadness            0.571   \n",
       "\n",
       "                                    aspect_terms          aspect_emotions  \n",
       "10998                        [#fail... #parents]              [NONE NONE]  \n",
       "3854   [@MagnetoBroHood, Thomas', group's focus]  [NONE, NONE, NONE NONE]  \n",
       "4493                              [worry, as, x]       [NONE, NONE, NONE]  \n",
       "7087                                [love, rage]             [NONE, NONE]  \n",
       "3010                                       [and]                   [NONE]  "
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_affect.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10156\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Affect Dimension</th>\n",
       "      <th>Intensity Score</th>\n",
       "      <th>aspect_terms</th>\n",
       "      <th>aspect_emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-En-01346</td>\n",
       "      <td>Life is too short to be jealous, hating, keepi...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.500</td>\n",
       "      <td>[to, hating,, that]</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-En-03526</td>\n",
       "      <td>@stephdavis77 Your parents should get you help...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.854</td>\n",
       "      <td>[Your, should, help]</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-En-41513</td>\n",
       "      <td>Sometimes the support network is causing the d...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.854</td>\n",
       "      <td>[network, the damage., #surivivor]</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-En-00995</td>\n",
       "      <td>'....trying to work out how a band featuring C...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.739</td>\n",
       "      <td>[how, Corin, and, could]</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-En-01068</td>\n",
       "      <td>WTF is happening with @nextofficial sale websi...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.953</td>\n",
       "      <td>[other, order]</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                              Tweet  \\\n",
       "0  2018-En-01346  Life is too short to be jealous, hating, keepi...   \n",
       "1  2018-En-03526  @stephdavis77 Your parents should get you help...   \n",
       "3  2017-En-41513  Sometimes the support network is causing the d...   \n",
       "4  2018-En-00995  '....trying to work out how a band featuring C...   \n",
       "5  2018-En-01068  WTF is happening with @nextofficial sale websi...   \n",
       "\n",
       "  Affect Dimension  Intensity Score                        aspect_terms  \\\n",
       "0             fear            0.500                 [to, hating,, that]   \n",
       "1             fear            0.854                [Your, should, help]   \n",
       "3          sadness            0.854  [network, the damage., #surivivor]   \n",
       "4          sadness            0.739            [how, Corin, and, could]   \n",
       "5            anger            0.953                      [other, order]   \n",
       "\n",
       "  aspect_emotions  \n",
       "0            none  \n",
       "1            none  \n",
       "3            none  \n",
       "4            none  \n",
       "5            none  "
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def uniform_emotions_filter(df):\n",
    "    # Define a function to check if all emotions in a list are the same\n",
    "    def is_uniform(emotion_list):\n",
    "        if not emotion_list:\n",
    "            return False\n",
    "        # Normalize the emotion strings to lowercase and split by spaces to handle cases like \"SADNESS SADNESS\"\n",
    "        normalized = [set(e.lower().split()) for e in emotion_list]\n",
    "        # Check if all sets in the list are equal to the first one\n",
    "        return all(e == normalized[0] for e in normalized)\n",
    "\n",
    "    # Apply the filter function to the DataFrame\n",
    "    filtered_df = df[df['aspect_emotions'].apply(is_uniform)].copy()\n",
    "\n",
    "    # Transform the 'aspect_emotions' column by reducing lists to a single lowercase word\n",
    "    def reduce_to_single_word(emotion_list):\n",
    "        if emotion_list:\n",
    "            # Take the first emotion and convert to lowercase, assuming all are the same based on the filter\n",
    "            emotion = emotion_list[0].split()[0].lower()\n",
    "            # Replace \"happiness\" with \"joy\"\n",
    "            return \"joy\" if emotion == \"happiness\" else emotion\n",
    "        return None\n",
    "\n",
    "    filtered_df['aspect_emotions'] = filtered_df['aspect_emotions'].apply(reduce_to_single_word)\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "filtered_df_affect= uniform_emotions_filter(df_affect)\n",
    "print(len(filtered_df_affect))\n",
    "filtered_df_affect.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_statistics(df):\n",
    "    # Get the unique emotion categories in the 'affect dimension' column\n",
    "    unique_dimensions = df['Affect Dimension'].unique()\n",
    "    \n",
    "    # Initialize a dictionary to store the statistics\n",
    "    stats = {}\n",
    "\n",
    "    # Loop through each unique emotion category in 'affect dimension'\n",
    "    for dimension in unique_dimensions:\n",
    "        # Filter the dataframe for the current dimension\n",
    "        subset = df[df['Affect Dimension'] == dimension]\n",
    "        \n",
    "        # Calculate the count of each 'affect emotion' within this dimension\n",
    "        emotion_counts = subset['aspect_emotions'].value_counts()\n",
    "        \n",
    "        # Calculate the percentage of each emotion\n",
    "        emotion_percentage = (emotion_counts / emotion_counts.sum() * 100).round(1)\n",
    "        \n",
    "        # Store the results in the stats dictionary\n",
    "        stats[dimension] = emotion_percentage\n",
    "\n",
    "    # Convert the stats dictionary to a DataFrame for better readability\n",
    "    stats_df = pd.DataFrame(stats)\n",
    "    \n",
    "    return stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fear</th>\n",
       "      <th>sadness</th>\n",
       "      <th>anger</th>\n",
       "      <th>joy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aspect_emotions</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>none</th>\n",
       "      <td>92.4</td>\n",
       "      <td>94.0</td>\n",
       "      <td>93.1</td>\n",
       "      <td>94.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>3.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 fear  sadness  anger   joy\n",
       "aspect_emotions                            \n",
       "anger             0.9      0.7    0.6   0.7\n",
       "fear              3.0      2.2    2.9   2.5\n",
       "joy               0.0      0.1    0.0   0.1\n",
       "none             92.4     94.0   93.1  94.2\n",
       "sadness           3.6      3.0    3.3   2.4"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = emotion_statistics(filtered_df_affect)\n",
    "\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_affect.to_csv(path + \"results_on_SemEvalAffect.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NONE': 17086, 'FEAR': 1326, 'SADNESS': 327, 'ANGER': 25, 'HAPPINESS': 3})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>aspect_terms</th>\n",
       "      <th>aspect_emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4447</th>\n",
       "      <td>delicious simple food in nice outdoor atmosphe...</td>\n",
       "      <td>[[17, 21, Positive], [38, 48, Positive]]</td>\n",
       "      <td>[simple, food]</td>\n",
       "      <td>[NONE, NONE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2459</th>\n",
       "      <td>This has got to be one of the most overrated r...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[This, restaurants]</td>\n",
       "      <td>[NONE, NONE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6977</th>\n",
       "      <td>No desert menu , no apology , nothing !!!!!!</td>\n",
       "      <td>[]</td>\n",
       "      <td>[desert menu, apology]</td>\n",
       "      <td>[NONE NONE, SADNESS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600</th>\n",
       "      <td>also make sure you pay attention to the music ...</td>\n",
       "      <td>[[40, 45, Neutral]]</td>\n",
       "      <td>[music, selection]</td>\n",
       "      <td>[NONE, NONE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>Paul , the maitre d' , was totally professiona...</td>\n",
       "      <td>[[0, 4, Positive]]</td>\n",
       "      <td>[Paul, maitre d']</td>\n",
       "      <td>[NONE, NONE NONE]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "4447  delicious simple food in nice outdoor atmosphe...   \n",
       "2459  This has got to be one of the most overrated r...   \n",
       "6977       No desert menu , no apology , nothing !!!!!!   \n",
       "2600  also make sure you pay attention to the music ...   \n",
       "978   Paul , the maitre d' , was totally professiona...   \n",
       "\n",
       "                                         label            aspect_terms  \\\n",
       "4447  [[17, 21, Positive], [38, 48, Positive]]          [simple, food]   \n",
       "2459                                        []     [This, restaurants]   \n",
       "6977                                        []  [desert menu, apology]   \n",
       "2600                       [[40, 45, Neutral]]      [music, selection]   \n",
       "978                         [[0, 4, Positive]]       [Paul, maitre d']   \n",
       "\n",
       "           aspect_emotions  \n",
       "4447          [NONE, NONE]  \n",
       "2459          [NONE, NONE]  \n",
       "6977  [NONE NONE, SADNESS]  \n",
       "2600          [NONE, NONE]  \n",
       "978      [NONE, NONE NONE]  "
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All Restaurant Datasets from SemEval \n",
    "\n",
    "emotion_word_counts_rest_all = count_emotion_words1(df_rest_all)\n",
    "print(emotion_word_counts_rest_all)\n",
    "\n",
    "df_rest_all.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect-level Emotions found by the Model in the Affect SemEval 2018 dataest:\t\tCounter({'NONE': 23021, 'SADNESS': 1162, 'FEAR': 987, 'ANGER': 254, 'HAPPINESS': 37})\n",
      "Sentence-level Emotions found by the Model in the Affect SemEval 2018 dataest:\t\tCounter({'none': 9484, 'sadness': 317, 'fear': 273, 'anger': 75, 'joy': 7})\n",
      "Emotions in the Affect SemEval 2018 dataest:\t\t\t\t\t\tCounter({'fear': 2938, 'anger': 2492, 'joy': 2409, 'sadness': 2317})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Affect Dimension</th>\n",
       "      <th>Intensity Score</th>\n",
       "      <th>aspect_terms</th>\n",
       "      <th>aspect_emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>2018-En-01297</td>\n",
       "      <td>Getting up for work is so much harder when Cha...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.520</td>\n",
       "      <td>[so much]</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6000</th>\n",
       "      <td>2018-En-01099</td>\n",
       "      <td>Are you always looking for quotes of your days...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.281</td>\n",
       "      <td>[of, Then]</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3080</th>\n",
       "      <td>2018-En-00380</td>\n",
       "      <td>@DebraHorrocks @Fash_Rev have some shocking st...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.350</td>\n",
       "      <td>[@DebraHorrocks, have, stats]</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4046</th>\n",
       "      <td>2018-En-02439</td>\n",
       "      <td>Hmmmm. Seems we only had 14 punters vote in #S...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.690</td>\n",
       "      <td>[punters]</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3170</th>\n",
       "      <td>2018-En-03570</td>\n",
       "      <td>This is heart-stopping, take-my-breath-away fu...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.797</td>\n",
       "      <td>[is]</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID                                              Tweet  \\\n",
       "872   2018-En-01297  Getting up for work is so much harder when Cha...   \n",
       "6000  2018-En-01099  Are you always looking for quotes of your days...   \n",
       "3080  2018-En-00380  @DebraHorrocks @Fash_Rev have some shocking st...   \n",
       "4046  2018-En-02439  Hmmmm. Seems we only had 14 punters vote in #S...   \n",
       "3170  2018-En-03570  This is heart-stopping, take-my-breath-away fu...   \n",
       "\n",
       "     Affect Dimension  Intensity Score                   aspect_terms  \\\n",
       "872           sadness            0.520                      [so much]   \n",
       "6000              joy            0.281                     [of, Then]   \n",
       "3080             fear            0.350  [@DebraHorrocks, have, stats]   \n",
       "4046          sadness            0.690                      [punters]   \n",
       "3170              joy            0.797                           [is]   \n",
       "\n",
       "     aspect_emotions  \n",
       "872             none  \n",
       "6000            none  \n",
       "3080            none  \n",
       "4046            none  \n",
       "3170            none  "
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SemEval Affect  2018\n",
    "\n",
    "print(f\"Aspect-level Emotions found by the Model in the Affect SemEval 2018 dataest:\\t\\t{count_emotion_words1(df_affect)}\")\n",
    "\n",
    "print(f\"Sentence-level Emotions found by the Model in the Affect SemEval 2018 dataest:\\t\\t{count_emotion_words2(filtered_df_affect, 'aspect_emotions')}\")\n",
    "\n",
    "print(f\"Emotions in the Affect SemEval 2018 dataest:\\t\\t\\t\\t\\t\\t{count_emotion_words2(filtered_df_affect, 'Affect Dimension')}\")\n",
    "\n",
    "filtered_df_affect.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\anaconda3\\envs\\absa-application\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "# Extract true labels and predicted labels\n",
    "true_labels = filtered_df_affect['Affect Dimension']\n",
    "predicted_labels = filtered_df_affect['aspect_emotions']\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels, average='macro')\n",
    "recall = recall_score(true_labels, predicted_labels, average='macro')\n",
    "f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "# Calculate metrics for each individual emotion\n",
    "precision_individual = precision_score(true_labels, predicted_labels, average=None, labels=['joy', 'anger', 'sadness', 'fear'])\n",
    "recall_individual = recall_score(true_labels, predicted_labels, average=None, labels=['joy', 'anger', 'sadness', 'fear'])\n",
    "f1_individual = f1_score(true_labels, predicted_labels, average=None, labels=['joy', 'anger', 'sadness', 'fear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Affect Dimension</th>\n",
       "      <th>Intensity Score</th>\n",
       "      <th>aspect_terms</th>\n",
       "      <th>aspect_emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9461</th>\n",
       "      <td>2018-En-00133</td>\n",
       "      <td>I think I'll eternally be irritated by our LIT...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.375</td>\n",
       "      <td>[irritated]</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5108</th>\n",
       "      <td>2017-En-30944</td>\n",
       "      <td>God hears your voice optimism at the moment th...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.312</td>\n",
       "      <td>[at, that, that]</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5334</th>\n",
       "      <td>2018-En-02214</td>\n",
       "      <td>When I think about Yondu &amp;amp; his crew, Rocke...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.547</td>\n",
       "      <td>[I, about]</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7640</th>\n",
       "      <td>2017-En-40237</td>\n",
       "      <td>@TalesofanAlfa @David_Milloy \\nI like your thi...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.604</td>\n",
       "      <td>[\\nI, sadly no]</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5559</th>\n",
       "      <td>2018-En-01652</td>\n",
       "      <td>I'm also wheezing really hard. I decided to ma...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.556</td>\n",
       "      <td>[I, and, after]</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID                                              Tweet  \\\n",
       "9461  2018-En-00133  I think I'll eternally be irritated by our LIT...   \n",
       "5108  2017-En-30944  God hears your voice optimism at the moment th...   \n",
       "5334  2018-En-02214  When I think about Yondu &amp; his crew, Rocke...   \n",
       "7640  2017-En-40237  @TalesofanAlfa @David_Milloy \\nI like your thi...   \n",
       "5559  2018-En-01652  I'm also wheezing really hard. I decided to ma...   \n",
       "\n",
       "     Affect Dimension  Intensity Score      aspect_terms aspect_emotions  \n",
       "9461             fear            0.375       [irritated]            none  \n",
       "5108              joy            0.312  [at, that, that]            none  \n",
       "5334              joy            0.547        [I, about]            none  \n",
       "7640          sadness            0.604   [\\nI, sadly no]            none  \n",
       "5559          sadness            0.556   [I, and, after]            none  "
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df_affect.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NONE': 23021, 'SADNESS': 1162, 'FEAR': 987, 'ANGER': 254, 'HAPPINESS': 37})\n",
      "1162\n",
      "Counter({'none': 9484, 'sadness': 317, 'fear': 273, 'anger': 75, 'joy': 7})\n",
      "317\n",
      "Counter({'fear': 2938, 'anger': 2492, 'joy': 2409, 'sadness': 2317})\n"
     ]
    }
   ],
   "source": [
    "print(count_emotion_words1(df_affect))\n",
    "print(count_emotion_words1(df_affect)['SADNESS'])\n",
    "print(count_emotion_words2(filtered_df_affect, 'aspect_emotions'))\n",
    "print(count_emotion_words2(filtered_df_affect, 'aspect_emotions')['sadness'])\n",
    "print(count_emotion_words2(filtered_df_affect, 'Affect Dimension'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained on GRACE/data/same_split_as_absa/out_ABEA_with_0.25DO_0.05_weight_decay_ateacs/pytorch_model.bin.8\n",
      "---------------\n",
      "  \n",
      "ASPECT TERM EXTRACTION VALIDATION\n",
      "\n",
      "Model performance for SemEval Restaurants All:\t\tPrecision: 0.5\tRecall: 0.8338\t\tF1: 0.6251\n",
      "\n",
      "Model performance for SemEval Laptop 2014:\t\tPrecision: 0.3265\tRecall: 0.6879\t\tF1: 0.4428\n",
      "---------------\n",
      "  \n",
      "ASPECT EMOTION CLASSIFICATION VALIDATION\n",
      "Aspect-level Emotions found by the Model in the Affect SemEval 2018 dataset:\t\tCounter({'NONE': 23021, 'SADNESS': 1162, 'FEAR': 987, 'ANGER': 254, 'HAPPINESS': 37})\n",
      "Sentence-level Emotions found by the Model in the Affect SemEval 2018 dataset:\t\tCounter({'none': 9484, 'sadness': 317, 'fear': 273, 'anger': 75, 'joy': 7})\n",
      "Actual Sentence-Level Emotions in the Affect SemEval 2018 dataset:\t\t\tCounter({'fear': 2938, 'anger': 2492, 'joy': 2409, 'sadness': 2317})\n",
      "   \n",
      "Overall Accuracy: 0.017\n",
      "Overall Precision: 0.234\n",
      "Overall Recall: 0.013\n",
      "Overall F1 Score: 0.024\n",
      "   \n",
      "\n",
      "Metrics for Joy:\n",
      "Precision: 0.429\n",
      "Recall: 0.001\n",
      "F1 Score: 0.002\n",
      "   \n",
      "\n",
      "Metrics for Anger:\n",
      "Precision: 0.2\n",
      "Recall: 0.006\n",
      "F1 Score: 0.012\n",
      "   \n",
      "\n",
      "Metrics for Sadness:\n",
      "Precision: 0.218\n",
      "Recall: 0.03\n",
      "F1 Score: 0.052\n",
      "   \n",
      "\n",
      "Metrics for Fear:\n",
      "Precision: 0.322\n",
      "Recall: 0.03\n",
      "F1 Score: 0.055\n",
      "   \n",
      "GRACE/data/same_split_as_absa/out_ABEA_with_0.25DO_0.05_weight_decay_ateacs/validation_stats.txt\n",
      "Mapping 'JOY' or 'JOY' to 'joy' with count: 0\n",
      "Mapping 'ANGER' or 'ANGER' to 'anger' with count: 508\n",
      "Mapping 'SADNESS' or 'SADNESS' to 'sadness' with count: 2324\n",
      "Mapping 'FEAR' or 'FEAR' to 'fear' with count: 1974\n",
      "Mapping 'JOY' or 'JOY' to 'joy' with count: 7\n",
      "Mapping 'ANGER' or 'ANGER' to 'anger' with count: 75\n",
      "Mapping 'SADNESS' or 'SADNESS' to 'sadness' with count: 317\n",
      "Mapping 'FEAR' or 'FEAR' to 'fear' with count: 273\n",
      "Mapping 'JOY' or 'JOY' to 'joy' with count: 2409\n",
      "Mapping 'ANGER' or 'ANGER' to 'anger' with count: 2492\n",
      "Mapping 'SADNESS' or 'SADNESS' to 'sadness' with count: 2317\n",
      "Mapping 'FEAR' or 'FEAR' to 'fear' with count: 2938\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "print(f\"Model trained on {model_path}\") \n",
    "print(\"---------------\")\n",
    "print(\"  \")\n",
    "\n",
    "print(\"ASPECT TERM EXTRACTION VALIDATION\")\n",
    "print(f\"\\nModel performance for SemEval Restaurants All:\\t\\tPrecision: {metrics_rest_all['Precision']}\\tRecall: {metrics_rest_all['Recall']}\\t\\tF1: {metrics_rest_all['F1 Score']}\")\n",
    "print(f\"\\nModel performance for SemEval Laptop 2014:\\t\\tPrecision: {metrics_lap14['Precision']}\\tRecall: {metrics_lap14['Recall']}\\t\\tF1: {metrics_lap14['F1 Score']}\")\n",
    "# print(f\"\\nModel performance for SemEval Restaurants 2014:\\t\\tPrecision: {metrics_rest14['Precision']}\\tRecall: {metrics_rest14['Recall']}\\t\\tF1: {metrics_rest14['F1 Score']}\")\n",
    "print(\"---------------\")\n",
    "print(\"  \")\n",
    "\n",
    "print(\"ASPECT EMOTION CLASSIFICATION VALIDATION\")\n",
    "print(f\"Aspect-level Emotions found by the Model in the Affect SemEval 2018 dataset:\\t\\t{count_emotion_words1(df_affect)}\")\n",
    "print(f\"Sentence-level Emotions found by the Model in the Affect SemEval 2018 dataset:\\t\\t{count_emotion_words2(filtered_df_affect, 'aspect_emotions')}\")\n",
    "print(f\"Actual Sentence-Level Emotions in the Affect SemEval 2018 dataset:\\t\\t\\t{count_emotion_words2(filtered_df_affect, 'Affect Dimension')}\")\n",
    "print(\"   \")\n",
    "\n",
    "# Print the results\n",
    "print(f\"Overall Accuracy: {round(accuracy,3)}\")\n",
    "print(f\"Overall Precision: {round(precision,3)}\")\n",
    "print(f\"Overall Recall: {round(recall,3)}\")\n",
    "print(f\"Overall F1 Score: {round(f1,3)}\")\n",
    "print(\"   \")\n",
    "\n",
    "# Print individual metrics\n",
    "for emotion, p, r, f in zip(['joy', 'anger', 'sadness', 'fear'], precision_individual, recall_individual, f1_individual):\n",
    "    print(f\"\\nMetrics for {emotion.capitalize()}:\")\n",
    "    print(f\"Precision: {round(p,3)}\")\n",
    "    print(f\"Recall: {round(r,3)}\")\n",
    "    print(f\"F1 Score: {round(f,3)}\")\n",
    "    print(\"   \")\n",
    "\n",
    "\n",
    "\n",
    "# Define a file name for the text output\n",
    "output_text_file = path + 'validation_stats.txt'\n",
    "print(output_text_file)\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(output_text_file, 'w') as f:\n",
    "    # Redirect prints to the file\n",
    "    f.write(f\"Model trained on {model_path}\\n\")\n",
    "    f.write(\"---------------\\n\\n\")\n",
    "    f.write(\"ASPECT TERM EXTRACTION VALIDATION\\n\")\n",
    "    f.write(f\"\\nModel performance for SemEval Restaurants All:\\t\\tPrecision: {metrics_rest_all['Precision']}\\tRecall: {metrics_rest_all['Recall']}\\t\\tF1: {metrics_rest_all['F1 Score']}\\n\")\n",
    "    f.write(f\"\\nModel performance for SemEval Laptop 2014:\\t\\tPrecision: {metrics_lap14['Precision']}\\tRecall: {metrics_lap14['Recall']}\\t\\tF1: {metrics_lap14['F1 Score']}\\n\")\n",
    "    # f.write(f\"\\nModel performance for SemEval Restaurants 2014:\\t\\tPrecision: {metrics_rest14['Precision']}\\tRecall: {metrics_rest14['Recall']}\\t\\tF1: {metrics_rest14['F1 Score']}\\n\")\n",
    "    f.write(\"---------------\\n\\n\")\n",
    "    f.write(\"ASPECT EMOTION CLASSIFICATION VALIDATION\\n\")\n",
    "    f.write(f\"Aspect-level Emotions found by the Model in the Affect SemEval 2018 dataset:\\t\\t{count_emotion_words1(df_affect)}\\n\")\n",
    "    f.write(f\"Sentence-level Emotions found by the Model in the Affect SemEval 2018 dataset:\\t\\t{count_emotion_words2(filtered_df_affect, 'aspect_emotions')}\\n\")\n",
    "    f.write(f\"Actual Sentence-Level Emotions in the Affect SemEval 2018 dataset:\\t\\t\\t{count_emotion_words2(filtered_df_affect, 'Affect Dimension')}\\n\\n\")\n",
    "    f.write(f\"Overall Accuracy: {round(accuracy, 3)}\\n\")\n",
    "    f.write(f\"Overall Precision: {round(precision, 3)}\\n\")\n",
    "    f.write(f\"Overall Recall: {round(recall, 3)}\\n\")\n",
    "    f.write(f\"Overall F1 Score: {round(f1, 3)}\\n\\n\")\n",
    "\n",
    "    # Print individual metrics\n",
    "    for emotion, p, r, f1 in zip(['joy', 'anger', 'sadness', 'fear'], precision_individual, recall_individual, f1_individual):\n",
    "        f.write(f\"\\nMetrics for {emotion.capitalize()}:\\n\")\n",
    "        f.write(f\"Precision: {round(p, 3)}\\n\")\n",
    "        f.write(f\"Recall: {round(r, 3)}\\n\")\n",
    "        f.write(f\"F1 Score: {round(f1, 3)}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create chart showing distributions\n",
    "\n",
    "# Normalized emotion labels to ensure consistency\n",
    "emotion_labels = ['joy', 'anger', 'sadness', 'fear']\n",
    "\n",
    "# Mapping to standard labels\n",
    "label_mapping = {\n",
    "    'HAPPINESS': 'joy',\n",
    "    'ANGER': 'anger',\n",
    "    'SADNESS': 'sadness',\n",
    "    'FEAR': 'fear',\n",
    "    'O': 'None',  # This can be ignored for now unless 'None' needs to be included in the plot\n",
    "}\n",
    "\n",
    "# Standardizing the counts according to the normalized labels\n",
    "def standardize_counts(counter, label_mapping, emotion_labels):\n",
    "    standardized_counts = []\n",
    "    for emotion in emotion_labels:\n",
    "        # Normalize label using the mapping\n",
    "        standard_label = label_mapping.get(emotion.upper(), emotion)\n",
    "        # Fetch the count from the counter, considering both the original and mapped label\n",
    "        count = counter.get(emotion, 0) + counter.get(emotion.upper(), 0) + counter.get(standard_label.upper(), 0)\n",
    "        print(f\"Mapping '{emotion.upper()}' or '{standard_label.upper()}' to '{standard_label}' with count: {count}\")\n",
    "        standardized_counts.append(count)\n",
    "    return standardized_counts\n",
    "\n",
    "# Standardize counts for all conditions\n",
    "standardized_counts1 = standardize_counts(count_emotion_words1(df_affect), label_mapping, emotion_labels)\n",
    "standardized_counts2 = standardize_counts(count_emotion_words2(filtered_df_affect, 'aspect_emotions'), label_mapping, emotion_labels)\n",
    "standardized_counts3 = standardize_counts(count_emotion_words2(filtered_df_affect, 'Affect Dimension'), label_mapping, emotion_labels)\n",
    "\n",
    "counts1 = [count_emotion_words1(df_affect)['HAPPINESS'], count_emotion_words1(df_affect)['ANGER'], count_emotion_words1(df_affect)['SADNESS'], count_emotion_words1(df_affect)['FEAR']]\n",
    "counts2 = [count_emotion_words2(filtered_df_affect, 'aspect_emotions')['joy'], count_emotion_words2(filtered_df_affect, 'aspect_emotions')['anger'], count_emotion_words2(filtered_df_affect, 'aspect_emotions')['sadness'], count_emotion_words2(filtered_df_affect, 'aspect_emotions')['fear']]\n",
    "counts3 = [count_emotion_words2(filtered_df_affect, 'Affect Dimension')['joy'], count_emotion_words2(filtered_df_affect, 'Affect Dimension')['anger'], count_emotion_words2(filtered_df_affect, 'Affect Dimension')['sadness'], count_emotion_words2(filtered_df_affect, 'Affect Dimension')['fear']]\n",
    "\n",
    "# Set up bar width and positions\n",
    "bar_width = 0.25\n",
    "index = np.arange(len(emotion_labels))\n",
    "\n",
    "# Custom colors for each group\n",
    "color1 = '#4e83e6'  # Aspect-level (light blue)\n",
    "color2 = '#144eba'  # Sentence-level (dark blue)\n",
    "color3 = '#3db853'  # Actual (Blue)\n",
    "\n",
    "# Plotting the bars with custom colors\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "bars1 = ax.bar(index, counts1, bar_width, label='Identified at Aspect-level', color=color1)\n",
    "bars2 = ax.bar(index + bar_width, counts2, bar_width, label='Aggregated at Sentence-level', color=color2)\n",
    "bars3 = ax.bar(index + 2 * bar_width, counts3, bar_width, label='Actual', color=color3)\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel('Emotion Labels')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Emotion Distributions for the SemEval Affect Dataset')\n",
    "ax.set_xticks(index + bar_width)\n",
    "ax.set_xticklabels(emotion_labels)\n",
    "ax.legend()\n",
    "\n",
    "# Add counts on top of the bars\n",
    "def add_bar_labels(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{int(height)}', \n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "add_bar_labels(bars1)\n",
    "add_bar_labels(bars2)\n",
    "add_bar_labels(bars3)\n",
    "\n",
    "# Save the figure as a PNG file\n",
    "output_image_file = path + 'emotion_distributions.png'\n",
    "plt.savefig(output_image_file)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "absa-application",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
