{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A47FQoHyad4C"
   },
   "source": [
    "# Annotations Data Postprocessing\n",
    "\n",
    "This notebook performs the following steps: \n",
    "\n",
    "1. <b>Load Annotated Data</b>\n",
    "   - Load the ```.jsonl``` files that were downloaded directly from the [Doccano labeling application](https://doccano.github.io/doccano/) \n",
    "\n",
    "2. <b>Merge Annotated Data </b>\n",
    "   - Merge each individual annotators results into a single DataFrame per annotator\n",
    "   - Remove any duplicates in the individual DataFrames\n",
    "   - Merge all annnotators results into a single DataFrame\n",
    "\n",
    "3. <b>Adjust for Exact Triplet Annotations</b>\n",
    "   - Check that each tweet (```message_id```)is contained in the merged results exactly 3 times (not just once or twice), indicating it was annotated 3x\n",
    "   - Drop any tweets that aren't triplets\n",
    "\n",
    "4. <b>Organize and Sort Data</b>\n",
    "   - Sort the DataFrame by ```message_id``` so that all tweet-triplets are next to each other\n",
    "\n",
    "5. <b>Inter-Annotator Agreement Analysis</b>\n",
    "   - Define a function to identify continuous ranges and the most frequent word within these ranges\n",
    "   - Initialize the final df for storing inter-annotator agreements\n",
    "   - Iterate over each unique 'message_id':\n",
    "       - Collect all annotations associated with the 'message_id'\n",
    "       - For each annotation, extract and process the annotated ranges and emotions \n",
    "       - Filter out annotations not supported by at least 2 annotators (including “none”)\n",
    "       - Apply the continuous ranges function to identify agreed-upon labels\n",
    "       - Update the final df with the agreed-upon labels\n",
    "\n",
    "6. <b>Save the Final Dataset</b>\n",
    "   - Save the df containing inter-annotator agreements to a CSV file\n",
    "\n",
    "7. <b>Reformat the Data into a .txt File for GRACE Model Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5AYQHKgMZuN"
   },
   "source": [
    "### Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tmfjUdi7aV6S"
   },
   "outputs": [],
   "source": [
    "# ------------- Data Handling -----------------------------------------\n",
    "import pandas as pd                                                     # Powerful data structures for data analysis, time series, and statistics\n",
    "from collections import Counter                                         # A container that keeps count of the elements in an iterable\n",
    "import ast                                                              # A Python module that helps to process trees of the Python abstract syntax grammar\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ------------- Plotting Data -----------------------------------------\n",
    "import matplotlib.pyplot as plt                                         # A plotting library for creating static, interactive, and animated visualizations in Python\n",
    "import matplotlib.ticker as ticker                                      # Provides classes for configuring tick locating and formatting\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_q5n51Q6dhZu"
   },
   "source": [
    "### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MoDHR7XBbLe9",
    "outputId": "1af21b9e-1a85-4864-8539-e55d85f0abc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first subset: 104\n",
      "second subset: 203\n",
      "third subset: 667\n",
      "fourth subset: 1170\n",
      "\n",
      "final, merged dataset: 2144\n",
      "\n",
      "duplicates removed: 1274\n"
     ]
    }
   ],
   "source": [
    "# @title Annotator 1\n",
    "\n",
    "a1_1 = pd.read_json(\"Data/Training Data/1 - Annotation Results/First 1K/1K_a1_1/aspect-emotion.jsonl\", lines=True)\n",
    "print(f'first subset: {len(a1_1)}')\n",
    "\n",
    "a1_2 = pd.read_json(\"Data/Training Data/1 - Annotation Results/First 1K/1K_a1_2/aspect-emotion.jsonl\", lines=True)\n",
    "print(f'second subset: {len(a1_2)}')\n",
    "\n",
    "a1_3 = pd.read_json(\"Data/Training Data/1 - Annotation Results/First 1K/1K_a1_3/aspect-emotion.jsonl\", lines=True)\n",
    "print(f'third subset: {len(a1_3)}')\n",
    "\n",
    "a1_4 = pd.read_json(\"Data/Training Data/1 - Annotation Results/First 1K/1K_a1_4/aspect-emotion.jsonl\", lines=True)\n",
    "print(f'fourth subset: {len(a1_4)}')\n",
    "\n",
    "\n",
    "# merge individual dfs into one for the annotator\n",
    "a1_with_duplicates = pd.concat([a1_1, a1_2, a1_3, a1_4], ignore_index=True)\n",
    "print(f'\\nfinal, merged dataset: {len(a1_with_duplicates)}')\n",
    "\n",
    "# remove any duplications (by \"message_id\")\n",
    "a1 = a1_with_duplicates.drop_duplicates(subset='message_id', keep='first')\n",
    "print(f'\\nduplicates removed: {len(a1)}')\n",
    "\n",
    "# save final annotated dataet for the annotator to a csv file\n",
    "# a1.to_csv(\"/content/drive/MyDrive/Msc AGI/Master/Jan 2024 ABEA Training Dataset Labeling/Final Annotations/annotator1.csv\", index=False)\n",
    "# print('Saved to drive.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A3msBKvXcd44",
    "outputId": "de42cb33-2124-452e-fe37-24a1305eee9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first subset: 104\n",
      "second subset: 104\n",
      "third subset: 667\n",
      "fourth subset: 1188\n",
      "\n",
      "final, merged dataset: 2063\n",
      "\n",
      "duplicates removed: 1292\n"
     ]
    }
   ],
   "source": [
    "# @title Annotator 2\n",
    "a2_1 = pd.read_json(\"Data/Training Data/1 - Annotation Results/First 1K/1K_a2_1/aspect-emotion.jsonl\", lines=True)\n",
    "print(f'first subset: {len(a2_1)}')\n",
    "\n",
    "a2_2 = pd.read_json(\"Data/Training Data/1 - Annotation Results/First 1K/1K_a2_2/aspect-emotion.jsonl\", lines=True)\n",
    "print(f'second subset: {len(a2_2)}')\n",
    "\n",
    "a2_3 = pd.read_json(\"Data/Training Data/1 - Annotation Results/First 1K/1K_a2_3/aspect-emotion.jsonl\", lines=True)\n",
    "print(f'third subset: {len(a2_3)}')\n",
    "\n",
    "a2_4 = pd.read_json(\"Data/Training Data/1 - Annotation Results/First 1K/1K_a2_4/aspect-emotion.jsonl\", lines=True)\n",
    "print(f'fourth subset: {len(a2_4)}')\n",
    "\n",
    "\n",
    "# merge individual dfs into one for the annotator\n",
    "a2_with_duplicates = pd.concat([a2_1, a2_2, a2_3, a2_4], ignore_index=True)\n",
    "print(f'\\nfinal, merged dataset: {len(a2_with_duplicates)}')\n",
    "\n",
    "# remove any duplications (by \"message_id\")\n",
    "a2 = a2_with_duplicates.drop_duplicates(subset='message_id', keep='first')\n",
    "print(f'\\nduplicates removed: {len(a2)}')\n",
    "\n",
    "# save final annotated dataet for the annotator to a csv file\n",
    "# a2.to_csv(\"/content/drive/MyDrive/Msc AGI/Master/Jan 2024 ABEA Training Dataset Labeling/Final Annotations/annotator2.csv\", index=False)\n",
    "# print('Saved to drive.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C_gDkihmhV-l",
    "outputId": "dee83bef-eb98-4aa9-b970-4a8e1c07b14f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first subset: 103\n",
      "second subset: 203\n",
      "third subset: 667\n",
      "fourth subset: 1170\n",
      "\n",
      "final, merged dataset: 2143\n",
      "\n",
      "duplicates removed: 1273\n"
     ]
    }
   ],
   "source": [
    "# @title Annotator 3\n",
    "a3_1 = pd.read_json(\"Data/Training Data/1 - Annotation Results/First 1K/1K_a3_1/aspect-emotion.jsonl\", lines=True)\n",
    "print(f'first subset: {len(a3_1)}')\n",
    "\n",
    "a3_2 = pd.read_json(\"Data/Training Data/1 - Annotation Results/First 1K/1K_a3_2/aspect-emotion.jsonl\", lines=True)\n",
    "print(f'second subset: {len(a3_2)}')\n",
    "\n",
    "a3_3 = pd.read_json(\"Data/Training Data/1 - Annotation Results/First 1K/1K_a3_3/aspect-emotion.jsonl\", lines=True)\n",
    "print(f'third subset: {len(a3_3)}')\n",
    "\n",
    "a3_4 = pd.read_json(\"Data/Training Data/1 - Annotation Results/First 1K/1K_a3_4/aspect-emotion.jsonl\", lines=True)\n",
    "print(f'fourth subset: {len(a3_4)}')\n",
    "\n",
    "\n",
    "# merge individual dfs into one for the annotator\n",
    "a3_with_duplicates = pd.concat([a3_1, a3_2, a3_3, a3_4], ignore_index=True)\n",
    "print(f'\\nfinal, merged dataset: {len(a3_with_duplicates)}')\n",
    "\n",
    "# remove any duplications (by \"message_id\")\n",
    "a3 = a3_with_duplicates.drop_duplicates(subset='message_id', keep='first')\n",
    "print(f'\\nduplicates removed: {len(a3)}')\n",
    "\n",
    "# save final annotated dataet for the annotator to a csv file\n",
    "# a3.to_csv(\"/content/drive/MyDrive/Msc AGI/Master/Jan 2024 ABEA Training Dataset Labeling/Final Annotations/annotator3.csv\", index=False)\n",
    "# print('Saved to drive.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OrJbzjwZhsDN",
    "outputId": "d49d86ba-a040-4746-ee66-d1edec665501"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first subset: 500\n",
      "second subset: 1000\n",
      "\n",
      "final, merged dataset: 1500\n",
      "\n",
      "duplicates removed: 1000\n"
     ]
    }
   ],
   "source": [
    "# @title Annotator 4\n",
    "a4_1 = pd.read_json(\"Data/Training Data/1 - Annotation Results/Second 1K/2K_a1_1/aspect-emotion.jsonl\", lines=True)\n",
    "print(f'first subset: {len(a4_1)}')\n",
    "\n",
    "a4_2 = pd.read_json(\"Data/Training Data/1 - Annotation Results/Second 1K/2K_a1_2/aspect-emotion.jsonl\", lines=True)\n",
    "print(f'second subset: {len(a4_2)}')\n",
    "\n",
    "\n",
    "# merge individual dfs into one for the annotator\n",
    "a4_with_duplicates = pd.concat([a4_1, a4_2], ignore_index=True)\n",
    "print(f'\\nfinal, merged dataset: {len(a4_with_duplicates)}')\n",
    "\n",
    "# remove any duplications (by \"message_id\")\n",
    "a4 = a4_with_duplicates.drop_duplicates(subset='message_id', keep='first')\n",
    "print(f'\\nduplicates removed: {len(a4)}')\n",
    "\n",
    "# save final annotated dataet for the annotator to a csv file\n",
    "# a4.to_csv(\"/content/drive/MyDrive/Msc AGI/Master/Jan 2024 ABEA Training Dataset Labeling/Final Annotations/annotator4.csv\", index=False)\n",
    "# print('Saved to drive.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Op4V7uuuiNtk",
    "outputId": "92611a26-ed8a-45dd-b001-1236aa893e60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first subset: 501\n",
      "second subset: 1000\n",
      "\n",
      "final, merged dataset: 1501\n",
      "\n",
      "duplicates removed: 1000\n"
     ]
    }
   ],
   "source": [
    "# @title Annotator 5\n",
    "a5_1 = pd.read_json(\"Data/Training Data/1 - Annotation Results/Second 1K/2K_a2_1/aspect-emotion.jsonl\", lines=True)\n",
    "print(f'first subset: {len(a5_1)}')\n",
    "\n",
    "a5_2 = pd.read_json(\"Data/Training Data/1 - Annotation Results/Second 1K/2K_a2_2/aspect-emotion.jsonl\", lines=True)\n",
    "print(f'second subset: {len(a5_2)}')\n",
    "\n",
    "\n",
    "# merge individual dfs into one for the annotator\n",
    "a5_with_duplicates = pd.concat([a5_1, a5_2], ignore_index=True)\n",
    "print(f'\\nfinal, merged dataset: {len(a5_with_duplicates)}')\n",
    "\n",
    "# remove any duplications (by \"message_id\")\n",
    "a5 = a5_with_duplicates.drop_duplicates(subset='message_id', keep='first')\n",
    "print(f'\\nduplicates removed: {len(a5)}')\n",
    "\n",
    "# save final annotated dataet for the annotator to a csv file\n",
    "# a5.to_csv(\"/content/drive/MyDrive/Msc AGI/Master/Jan 2024 ABEA Training Dataset Labeling/Final Annotations/annotator5.csv\", index=False)\n",
    "# print('Saved to drive.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FmoGxzZZiid8",
    "outputId": "f2d58c51-3220-46b2-954c-8469f92d541a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first subset: 500\n",
      "second subset: 1000\n",
      "\n",
      "final, merged dataset: 1500\n",
      "\n",
      "duplicates removed: 1000\n"
     ]
    }
   ],
   "source": [
    "# @title Annotator 6\n",
    "\n",
    "a6_1 = pd.read_json(\"Data/Training Data/1 - Annotation Results/Second 1K/2K_a3_1/aspect-emotion.jsonl\", lines=True)\n",
    "print(f'first subset: {len(a6_1)}')\n",
    "\n",
    "a6_2 = pd.read_json(\"Data/Training Data/1 - Annotation Results/Second 1K/2K_a3_2/aspect-emotion.jsonl\", lines=True)\n",
    "print(f'second subset: {len(a6_2)}')\n",
    "\n",
    "\n",
    "# merge individual dfs into one for the annotator\n",
    "a6_with_duplicates = pd.concat([a6_1, a6_2], ignore_index=True)\n",
    "print(f'\\nfinal, merged dataset: {len(a6_with_duplicates)}')\n",
    "\n",
    "# remove any duplications (by \"message_id\")\n",
    "a6 = a6_with_duplicates.drop_duplicates(subset='message_id', keep='first')\n",
    "print(f'\\nduplicates removed: {len(a6)}')\n",
    "\n",
    "# save final annotated dataet for the annotator to a csv file\n",
    "# a6.to_csv(\"/content/drive/MyDrive/Msc AGI/Master/Jan 2024 ABEA Training Dataset Labeling/Final Annotations/annotator6.csv\", index=False)\n",
    "# print('Saved to drive.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lOpWDEQQi5pE",
    "outputId": "6344cc65-ffb2-4634-ffcc-0285a7eca799"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first subset: 501\n"
     ]
    }
   ],
   "source": [
    "# @title Annotator 7\n",
    "a7 = pd.read_json(\"Data/Training Data/1 - Annotation Results/Third 1K/3K_a1/aspect-emotion.jsonl\", lines=True)\n",
    "print(f'first subset: {len(a7)}')\n",
    "\n",
    "# save final annotated dataet for the annotator to a csv file\n",
    "# a7.to_csv(\"/content/drive/MyDrive/Msc AGI/Master/Jan 2024 ABEA Training Dataset Labeling/Final Annotations/annotator7.csv\", index=False)\n",
    "# print('Saved to drive.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IyFivTdYj1OE",
    "outputId": "b24c449c-9148-4924-f2d1-8f56a6a41476"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first subset: 501\n"
     ]
    }
   ],
   "source": [
    "# @title Annotator 8\n",
    "a8 = pd.read_json(\"Data/Training Data/1 - Annotation Results/Third 1K/3K_a2/aspect-emotion.jsonl\", lines=True)\n",
    "print(f'first subset: {len(a8)}')\n",
    "\n",
    "# save final annotated dataet for the annotator to a csv file\n",
    "# a8.to_csv(\"/content/drive/MyDrive/Msc AGI/Master/Jan 2024 ABEA Training Dataset Labeling/Final Annotations/annotator8.csv\", index=False)\n",
    "# print('Saved to drive.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x4Pg1zrDj-Rx",
    "outputId": "7de3d455-5662-401c-af6b-6a76b9ccfb40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first subset: 500\n"
     ]
    }
   ],
   "source": [
    "# @title Annotator 9\n",
    "a9 = pd.read_json(\"Data/Training Data/1 - Annotation Results/Third 1K/3K_a3/aspect-emotion.jsonl\", lines=True)\n",
    "print(f'first subset: {len(a9)}')\n",
    "\n",
    "# save final annotated dataet for the annotator to a csv file\n",
    "# a9.to_csv(\"/content/drive/MyDrive/Msc AGI/Master/Jan 2024 ABEA Training Dataset Labeling/Final Annotations/annotator9.csv\", index=False)\n",
    "# print('Saved to drive.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncsyqLaskQbc"
   },
   "source": [
    "### Check how many tweets were annotated overall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U-xDP7Z5kThp",
    "outputId": "f69e8f49-b0d5-422a-c25d-aa84b5792f56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "final, merged dataset: 8341\n",
      "\n",
      "duplicates removed: 2800\n"
     ]
    }
   ],
   "source": [
    "# @title All tweets that were annotated\n",
    "\n",
    "# merge ALL individual dfs into one\n",
    "all_tweets_annotated_w_duplicates = pd.concat([a1, a2, a3, a4, a5, a6, a7, a8, a9], ignore_index=True)\n",
    "print(f'\\nfinal, merged dataset: {len(all_tweets_annotated_w_duplicates)}')\n",
    "\n",
    "# remove any duplications (by \"message_id\")\n",
    "all_tweets_annotated = all_tweets_annotated_w_duplicates.drop_duplicates(subset='message_id', keep='first')\n",
    "print(f'\\nduplicates removed: {len(all_tweets_annotated)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vqn-SsE8lQqj",
    "outputId": "537809c6-b3cd-4b51-ceed-648c30bfbe26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total, merged dataset: 8341\n",
      "\n",
      "All tweets, where the tweet was annotated by 3 annotators: 8298\n",
      "\n",
      "Rows with exactly 3 duplicates, then removed all duplicates: 2766\n"
     ]
    }
   ],
   "source": [
    "# @title All tweets that were annotated (adjusted for exact 3 duplicates)\n",
    "\n",
    "# merge ALL individual dfs into one\n",
    "all_tweets_annotated_w_duplicates = pd.concat([a1, a2, a3, a4, a5, a6, a7, a8, a9], ignore_index=True)\n",
    "print(f'total, merged dataset: {len(all_tweets_annotated_w_duplicates)}')\n",
    "\n",
    "# Group by 'message_id' and filter groups that have exactly 3 entries\n",
    "all_tweets_annotated_temp = all_tweets_annotated_w_duplicates.groupby('message_id').filter(lambda x: len(x) == 3)\n",
    "print(f'\\nAll tweets, where the tweet was annotated by 3 annotators: {len(all_tweets_annotated_temp)}')\n",
    "\n",
    "# save df with all tweets where there are 3 duplicates of each message_id as \"all_3_duplicate_tweets.csv\"\n",
    "# all_tweets_annotated_temp.to_csv(\"/content/drive/MyDrive/Msc AGI/Master/Jan 2024 ABEA Training Dataset Labeling/Final Annotations/all_tweets_w_three_duplicates.csv\", index=False)\n",
    "\n",
    "# Now, drop duplicates to keep only one instance of each 'message_id'\n",
    "all_tweets_annotated = all_tweets_annotated_temp.drop_duplicates(subset='message_id', keep='first')\n",
    "\n",
    "# reset the index for cleanliness\n",
    "all_tweets_annotated.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f'\\nRows with exactly 3 duplicates, then removed all duplicates: {len(all_tweets_annotated)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QCM3IrIyTKd"
   },
   "source": [
    "### 2. - 4. Sorted DF with all 3 annotations\n",
    "\n",
    "(all 3 annotations are next to grouped together in the df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Xxkqn0KnJR0",
    "outputId": "7b8eb031-2e88-447d-a663-11100d50c7b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8298\n"
     ]
    }
   ],
   "source": [
    "total_annotations = all_tweets_annotated_temp\n",
    "\n",
    "print(len(total_annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "JP8_ZFyjzm6a",
    "outputId": "acddf978-26b6-45a1-b882-d46f1d1968ee"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>message_id</th>\n",
       "      <th>label</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5521</td>\n",
       "      <td>01.09.20\\n\\nAfter a handful of days off, the l...</td>\n",
       "      <td>1300734393309331456</td>\n",
       "      <td>[[39, 49, Anger], [95, 98, Anger]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8185</td>\n",
       "      <td>01.09.20\\n\\nAfter a handful of days off, the l...</td>\n",
       "      <td>1300734393309331456</td>\n",
       "      <td>[[39, 49, Anger], [95, 98, Anger]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6853</td>\n",
       "      <td>01.09.20\\n\\nAfter a handful of days off, the l...</td>\n",
       "      <td>1300734393309331456</td>\n",
       "      <td>[[43, 50, Anger], [95, 99, Anger]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11120</td>\n",
       "      <td>Leopard? Spots? Wrong https://t.co/jZZpyQWUFi</td>\n",
       "      <td>1301600936926871552</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13786</td>\n",
       "      <td>Leopard? Spots? Wrong https://t.co/jZZpyQWUFi</td>\n",
       "      <td>1301600936926871552</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12453</td>\n",
       "      <td>Leopard? Spots? Wrong https://t.co/jZZpyQWUFi</td>\n",
       "      <td>1301600936926871552</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1895</td>\n",
       "      <td>The new breakfast line up. https://t.co/Lof0Dn...</td>\n",
       "      <td>1301784141223071744</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>562</td>\n",
       "      <td>The new breakfast line up. https://t.co/Lof0Dn...</td>\n",
       "      <td>1301784141223071744</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3228</td>\n",
       "      <td>The new breakfast line up. https://t.co/Lof0Dn...</td>\n",
       "      <td>1301784141223071744</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5558</td>\n",
       "      <td>@robdgill Not to mention income multiples</td>\n",
       "      <td>1301946973088616448</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6890</td>\n",
       "      <td>@robdgill Not to mention income multiples</td>\n",
       "      <td>1301946973088616448</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8222</td>\n",
       "      <td>@robdgill Not to mention income multiples</td>\n",
       "      <td>1301946973088616448</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8061</td>\n",
       "      <td>5. Sprinkle the rest of your pepperoni on top....</td>\n",
       "      <td>1301987663591940096</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5397</td>\n",
       "      <td>5. Sprinkle the rest of your pepperoni on top....</td>\n",
       "      <td>1301987663591940096</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6729</td>\n",
       "      <td>5. Sprinkle the rest of your pepperoni on top....</td>\n",
       "      <td>1301987663591940096</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text  \\\n",
       "0    5521  01.09.20\\n\\nAfter a handful of days off, the l...   \n",
       "1    8185  01.09.20\\n\\nAfter a handful of days off, the l...   \n",
       "2    6853  01.09.20\\n\\nAfter a handful of days off, the l...   \n",
       "3   11120      Leopard? Spots? Wrong https://t.co/jZZpyQWUFi   \n",
       "4   13786      Leopard? Spots? Wrong https://t.co/jZZpyQWUFi   \n",
       "5   12453      Leopard? Spots? Wrong https://t.co/jZZpyQWUFi   \n",
       "6    1895  The new breakfast line up. https://t.co/Lof0Dn...   \n",
       "7     562  The new breakfast line up. https://t.co/Lof0Dn...   \n",
       "8    3228  The new breakfast line up. https://t.co/Lof0Dn...   \n",
       "9    5558          @robdgill Not to mention income multiples   \n",
       "10   6890          @robdgill Not to mention income multiples   \n",
       "11   8222          @robdgill Not to mention income multiples   \n",
       "12   8061  5. Sprinkle the rest of your pepperoni on top....   \n",
       "13   5397  5. Sprinkle the rest of your pepperoni on top....   \n",
       "14   6729  5. Sprinkle the rest of your pepperoni on top....   \n",
       "\n",
       "             message_id                               label Comments  \n",
       "0   1300734393309331456  [[39, 49, Anger], [95, 98, Anger]]       []  \n",
       "1   1300734393309331456  [[39, 49, Anger], [95, 98, Anger]]       []  \n",
       "2   1300734393309331456  [[43, 50, Anger], [95, 99, Anger]]       []  \n",
       "3   1301600936926871552                                  []       []  \n",
       "4   1301600936926871552                                  []       []  \n",
       "5   1301600936926871552                                  []       []  \n",
       "6   1301784141223071744                                  []       []  \n",
       "7   1301784141223071744                                  []       []  \n",
       "8   1301784141223071744                                  []       []  \n",
       "9   1301946973088616448                                  []       []  \n",
       "10  1301946973088616448                                  []       []  \n",
       "11  1301946973088616448                                  []       []  \n",
       "12  1301987663591940096                                  []       []  \n",
       "13  1301987663591940096                                  []       []  \n",
       "14  1301987663591940096                                  []       []  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the DataFrame by 'message_id'\n",
    "sorted_df = total_annotations.sort_values(by='message_id')\n",
    "\n",
    "# Reset the index after sorting if needed\n",
    "sorted_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# sorted_df.to_csv(\"Data/Training Data/1 - Annotation Results/Final Results/all_tweets_w_three_duplicates_sorted.csv\", index=False)\n",
    "sorted_df.head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Check for Inter Annotator Agreements and Add to Final DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dominant_emotion(emotions):\n",
    "    '''\n",
    "    Determines the dominant emotion from a list of emotions. If there is no single dominant emotion, returns None\n",
    "    '''\n",
    "    emotion_counts = Counter(emotions).most_common(3)\n",
    "    if len(emotion_counts) > 1 and emotion_counts[0][1] == emotion_counts[1][1]:\n",
    "        return None                                                                     # No dominant emotion if the top two are equal\n",
    "    return emotion_counts[0][0]                                                         # Return the emotion with the highest count\n",
    "\n",
    "# function to check for overlapping index ranges\n",
    "def find_cont_ranges_and_dom_emotions(d):\n",
    "    '''\n",
    "    Function that takes a dictionary as input, where the keys are character indices (e.g. 1) and the values are a list of emotions that were annotated for that character,\n",
    "    and finds continuous character ranges (e.g. for a word or several words) and determines what the dominant emotion is that was labeled by 3 annotators\n",
    "    Input: dictionary with keys that are character indexes and values that is a list of any/all labeled emotions for that character\n",
    "    Output: A list of tuples with (start_idx, end_idx, emotion), where the range and the emotion are agreen upon by at least 2 annotators\n",
    "    '''\n",
    "\n",
    "    sorted_keys = sorted(d.keys())                                                      # Sort input dict by key values, which are character indexes\n",
    "    ranges = []                                                                         # Init a list to store continuous character ranges\n",
    "\n",
    "    current_range_start = sorted_keys[0]                                                # Begin at first index\n",
    "    current_range_end = current_range_start                                             # Set end index to first index, but update as we go\n",
    "    emotions_for_this_range = d[sorted_keys[0]]                                         # Init list for storing all emotions associated with this range of characters, and add first emotions\n",
    "\n",
    "    for i in range(1, len(sorted_keys)):                                                # Iterate over each of the character indexes\n",
    "        if sorted_keys[i] == sorted_keys[i - 1] + 1:                                    # Check if the current key continues the range\n",
    "            current_range_end = sorted_keys[i]                                          # Update range end to current index\n",
    "            emotions_for_this_range.extend(d[sorted_keys[i]])                           # Add the list of emotions from this character to the \"all emotions\" list\n",
    "        else:                                                                           # If the current key doesn't continue the current range (meaning end of current annotated range is reached)\n",
    "            dominant_emotion = find_dominant_emotion(emotions_for_this_range)           # Get the dominant emotion for this annotated range\n",
    "            if dominant_emotion:                                                        # If not none, add to final results list\n",
    "                ranges.append([current_range_start, current_range_end, dominant_emotion])\n",
    "            current_range_start = current_range_end = sorted_keys[i]                    # Update start index \n",
    "            emotions_for_this_range = d[sorted_keys[i]]                                 # Get current emotion labels\n",
    "\n",
    "    # Handle the last range\n",
    "    dominant_emotion = find_dominant_emotion(emotions_for_this_range)\n",
    "    if dominant_emotion:\n",
    "        ranges.append([current_range_start, current_range_end, dominant_emotion])\n",
    "\n",
    "    return ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CEWHKxqvZUz9",
    "outputId": "42e582cd-8af1-42ab-f82a-8f352a0d262b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_12004\\3525829615.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_iaa_df['label'] = ''\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302377117951369216\n",
      "1302407703051022336\n",
      "1322845152612933632\n",
      "1323285372131004416\n",
      "1327893319914020864\n",
      "1477117401145610240\n",
      "Total annotations: 51962\n",
      "Total annotations dropped: 5704\n",
      "{'iaa_2': 7, 'iaa_3': 4040}\n"
     ]
    }
   ],
   "source": [
    "# Set up final DF and add column for final label (which annotators agreed on)\n",
    "final_iaa_df = all_tweets_annotated\n",
    "final_iaa_df['label'] = ''\n",
    "\n",
    "# Iterate through groups of message_ids (= always 3 tweets, one from each annotator), get the three current labels\n",
    "for message_id, group in sorted_df.groupby('message_id'): \n",
    "  iaa_dict = {}                                                   # Initialise a dict for current tweet\n",
    "\n",
    "  for i in range(3):                                              # For each annotator, get their label, i.e. annotated ranges\n",
    "    annotator_label = group.iloc[i]['label']                      # e.g. [[0, 21, 'Happiness'], [128, 142, 'Sadness'], [205, 214, 'Sadness']]\n",
    "\n",
    "    if len(annotator_label) == 0:                                 # If the annotator didn't annotate anything, acknowledge this by setting pseudo index and emotion=('none')\n",
    "      start = -2\n",
    "      end = -1\n",
    "      emotion =  'none'\n",
    "\n",
    "      wholerange = list(range(start, end+1))                      # List of the pseudo range [-2, -1, 0]\n",
    "     \n",
    "      for k in range(len(wholerange)):                            # For each pseudo index number, add the nr and 'none' emotion to the dict\n",
    "        index_num = wholerange[k]                                 # Get current index number, e.g. -2\n",
    "        if index_num in iaa_dict:                                 # If index is already in dict, just add to the emotion label, else add index and emotion label\n",
    "          iaa_dict[index_num].append(emotion)\n",
    "        else:\n",
    "          iaa_dict[index_num] = [emotion]\n",
    "\n",
    "    else:                                                         # If the annotator did annotation something\n",
    "      for j in range(len(annotator_label)):                       # Loop over each annotated range (such as one word) in the sub lists by accessing length (e.g. 3 for the example above)\n",
    "        start = annotator_label[j][0]                             # e.g. 0\n",
    "        end = annotator_label[j][1]                               # e.g. 21\n",
    "        emotion = annotator_label[j][2]                           # e.g. 'Happiness'\n",
    "\n",
    "        wholerange = list(range(start, end+1))                    # List of the range, e.g. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
    "        \n",
    "        for k in range(len(wholerange)):                          # For each index number, add the nr and emotion to the dict\n",
    "          index_num = wholerange[k]\n",
    "          if index_num in iaa_dict:\n",
    "            iaa_dict[index_num].append(emotion)\n",
    "          else:\n",
    "            iaa_dict[index_num] = [emotion]\n",
    "\n",
    "\n",
    "  iaa_dict = {k: v for k, v in iaa_dict.items() if len(v) > 1}    # After collecting all annotated range numbers and emotions, drop any key-value pairs with only 1 emotion (= only one annotator marked it)\n",
    "\n",
    "  if iaa_dict != {}:                                              # If there are any annotations left \n",
    "    ranges = find_cont_ranges_and_dom_emotions(iaa_dict)          # Get the ranges and the dominant emotion\n",
    "    final_iaa_df.loc[final_iaa_df['message_id'] == message_id, 'label'] = str(ranges)     # add the final ranges and emotions to the 'label' column in the final df\n",
    "  else:                                                           # if there are no emotions then just add empty list brackets in the label column\n",
    "    final_iaa_df.loc[final_iaa_df['message_id'] == message_id, 'label'] = str('[]')\n",
    "\n",
    "# Save to CSV\n",
    "final_iaa_df.to_csv(\"Data/Training Data/1 - Annotation Results/Final Results/final_iaa_df_new.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dominant_emotion(emotions, iaa_counts):\n",
    "    '''\n",
    "    Determines the dominant emotion from a list of emotions. If there is no single dominant emotion, returns None\n",
    "    '''\n",
    "    emotion_counts = Counter(emotions).most_common(3)   \n",
    "    if len(emotion_counts) > 1 and emotion_counts[0][1] == emotion_counts[1][1]:\n",
    "        iaa_counts['iaa_2'] += 1 \n",
    "        return None, 1                                                # No dominant emotion if the top two are equal\n",
    "    iaa_counts['iaa_3'] += 1 \n",
    "    return emotion_counts[0][0] , 0                                 # Return the emotion with the highest count\n",
    "\n",
    "# function to check for overlapping index ranges\n",
    "def find_cont_ranges_and_dom_emotions(d, iaa_counts):\n",
    "    '''\n",
    "    Function that takes a dictionary as input, where the keys are character indices (e.g. 1) and the values are a list of emotions that were annotated for that character,\n",
    "    and finds continuous character ranges (e.g. for a word or several words) and determines what the dominant emotion is that was labeled by 3 annotators\n",
    "    Input: dictionary with keys that are character indexes and values that is a list of any/all labeled emotions for that character\n",
    "    Output: A list of tuples with (start_idx, end_idx, emotion), where the range and the emotion are agreen upon by at least 2 annotators\n",
    "    '''\n",
    "\n",
    "    sorted_keys = sorted(d.keys())                                                      # Sort input dict by key values, which are character indexes\n",
    "    ranges = []                                                                         # Init a list to store continuous character ranges\n",
    "\n",
    "    current_range_start = sorted_keys[0]                                                # Begin at first index\n",
    "    current_range_end = current_range_start                                             # Set end index to first index, but update as we go\n",
    "    emotions_for_this_range = d[sorted_keys[0]]                                         # Init list for storing all emotions associated with this range of characters, and add first emotions\n",
    "    get_msg_id = 0\n",
    "\n",
    "    for i in range(1, len(sorted_keys)):                                                # Iterate over each of the character indexes\n",
    "        if sorted_keys[i] == sorted_keys[i - 1] + 1:                                    # Check if the current key continues the range\n",
    "            current_range_end = sorted_keys[i]                                          # Update range end to current index\n",
    "            emotions_for_this_range.extend(d[sorted_keys[i]])                           # Add the list of emotions from this character to the \"all emotions\" list\n",
    "        else:                                                                           # If the current key doesn't continue the current range (meaning end of current annotated range is reached)\n",
    "            dominant_emotion, get_msg_id = find_dominant_emotion(emotions_for_this_range, iaa_counts)           # Get the dominant emotion for this annotated range\n",
    "            if dominant_emotion:                                                        # If not none, add to final results list\n",
    "                ranges.append([current_range_start, current_range_end, dominant_emotion])\n",
    "            current_range_start = current_range_end = sorted_keys[i]                    # Update start index \n",
    "            emotions_for_this_range = d[sorted_keys[i]]                                 # Get current emotion labels\n",
    "\n",
    "    # Handle the last range\n",
    "    dominant_emotion, get_msg_id = find_dominant_emotion(emotions_for_this_range, iaa_counts)\n",
    "    if dominant_emotion:\n",
    "        ranges.append([current_range_start, current_range_end, dominant_emotion])\n",
    "\n",
    "    return ranges, get_msg_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_12004\\1938912995.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_iaa_df['label'] = ''\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id  \\\n",
      "738  3177   \n",
      "739  1844   \n",
      "740   511   \n",
      "\n",
      "                                                                                             text  \\\n",
      "738  @pallious In the old days the Guardian would be all over a case like this. Nowadays.....nah.   \n",
      "739  @pallious In the old days the Guardian would be all over a case like this. Nowadays.....nah.   \n",
      "740  @pallious In the old days the Guardian would be all over a case like this. Nowadays.....nah.   \n",
      "\n",
      "              message_id                                   label Comments  \n",
      "738  1302377117951369216      [[26, 38, Anger], [75, 83, Anger]]       []  \n",
      "739  1302377117951369216                     [[26, 38, Sadness]]       []  \n",
      "740  1302377117951369216  [[26, 38, Sadness], [75, 83, Sadness]]       []  \n",
      "1302377117951369216\n",
      "        id  \\\n",
      "789  25355   \n",
      "790  26688   \n",
      "791  28021   \n",
      "\n",
      "                                                                                                                                             text  \\\n",
      "789  UK’s Brexit negotiator confirms the government took liberties with the truth when they tweeted this on 3 Dec 2019 ⬇️ https://t.co/c93miRiIRf   \n",
      "790  UK’s Brexit negotiator confirms the government took liberties with the truth when they tweeted this on 3 Dec 2019 ⬇️ https://t.co/c93miRiIRf   \n",
      "791  UK’s Brexit negotiator confirms the government took liberties with the truth when they tweeted this on 3 Dec 2019 ⬇️ https://t.co/c93miRiIRf   \n",
      "\n",
      "              message_id                label Comments  \n",
      "789  1302407703051022336    [[32, 46, Anger]]       []  \n",
      "790  1302407703051022336                   []       []  \n",
      "791  1302407703051022336  [[32, 46, Sadness]]       []  \n",
      "1302407703051022336\n",
      "        id  \\\n",
      "1722  8073   \n",
      "1723  6741   \n",
      "1724  5409   \n",
      "\n",
      "                                                                                                                                            text  \\\n",
      "1722  Business not as usual but @arla_uk agents like @3oaksestates will continue to follow even stricter government guidelines during #Lockdown2   \n",
      "1723  Business not as usual but @arla_uk agents like @3oaksestates will continue to follow even stricter government guidelines during #Lockdown2   \n",
      "1724  Business not as usual but @arla_uk agents like @3oaksestates will continue to follow even stricter government guidelines during #Lockdown2   \n",
      "\n",
      "               message_id                 label Comments  \n",
      "1722  1322845152612933632  [[99, 120, Sadness]]       []  \n",
      "1723  1322845152612933632    [[99, 120, Anger]]       []  \n",
      "1724  1322845152612933632     [[0, 8, Sadness]]       []  \n",
      "1322845152612933632\n",
      "        id  \\\n",
      "3003  8107   \n",
      "3004  6775   \n",
      "3005  5443   \n",
      "\n",
      "                                                                                                                           text  \\\n",
      "3003  @SarBritcliffeMP historical cases of racially motivated physical and verbal abuse in the British Army should be addressed   \n",
      "3004  @SarBritcliffeMP historical cases of racially motivated physical and verbal abuse in the British Army should be addressed   \n",
      "3005  @SarBritcliffeMP historical cases of racially motivated physical and verbal abuse in the British Army should be addressed   \n",
      "\n",
      "               message_id                                label Comments  \n",
      "3003  1323285372131004416    [[0, 16, Anger], [17, 33, Anger]]       []  \n",
      "3004  1323285372131004416  [[17, 33, Anger], [89, 101, Anger]]       []  \n",
      "3005  1323285372131004416                 [[89, 101, Sadness]]       []  \n",
      "1323285372131004416\n",
      "        id  \\\n",
      "3687  1897   \n",
      "3688   564   \n",
      "3689  3230   \n",
      "\n",
      "                                                                          text  \\\n",
      "3687  @SteveAs10397311 Well thats set back race relations decades! Well done 🙄   \n",
      "3688  @SteveAs10397311 Well thats set back race relations decades! Well done 🙄   \n",
      "3689  @SteveAs10397311 Well thats set back race relations decades! Well done 🙄   \n",
      "\n",
      "               message_id                                   label Comments  \n",
      "3687  1327893319914020864                       [[22, 26, Anger]]       []  \n",
      "3688  1327893319914020864  [[22, 26, Sadness], [37, 51, Sadness]]       []  \n",
      "3689  1327893319914020864                       [[28, 59, Anger]]       []  \n",
      "1327893319914020864\n",
      "        id  \\\n",
      "6279   495   \n",
      "6280  3161   \n",
      "6281  1828   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                  text  \\\n",
      "6279  Still thinking about this video that may  have captured the source of the grass fires that destroyed more than 500 homes in Boulder County.\\n\\nLocals tweeting it’s the site of the Twelve Tribes religious commune. Some have seen open fires burning on the property recently.   \n",
      "6280  Still thinking about this video that may  have captured the source of the grass fires that destroyed more than 500 homes in Boulder County.\\n\\nLocals tweeting it’s the site of the Twelve Tribes religious commune. Some have seen open fires burning on the property recently.   \n",
      "6281  Still thinking about this video that may  have captured the source of the grass fires that destroyed more than 500 homes in Boulder County.\\n\\nLocals tweeting it’s the site of the Twelve Tribes religious commune. Some have seen open fires burning on the property recently.   \n",
      "\n",
      "               message_id                label Comments  \n",
      "6279  1477117401145610240     [[21, 31, Fear]]       []  \n",
      "6280  1477117401145610240     [[74, 85, Fear]]       []  \n",
      "6281  1477117401145610240  [[21, 31, Sadness]]       []  \n",
      "1477117401145610240\n",
      "Total annotations: 51962\n",
      "Total annotations dropped: 5704\n",
      "{'iaa_2': 7, 'iaa_3': 4040}\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = None  # Set to None to display full column width without truncation\n",
    "\n",
    "\n",
    "# Set up final DF and add column for final label (which annotators agreed on)\n",
    "final_iaa_df = all_tweets_annotated\n",
    "final_iaa_df['label'] = ''\n",
    "\n",
    "total_annotations_count = 0\n",
    "total_annotations_dropped = 0\n",
    "iaa_counts = {'iaa_2': 0, 'iaa_3': 0}\n",
    "\n",
    "# Iterate through groups of message_ids (= always 3 tweets, one from each annotator), get the three current labels\n",
    "for message_id, group in sorted_df.groupby('message_id'): \n",
    "  \n",
    "  if message_id in [1302377117951369216, 1302407703051022336, 1322845152612933632, 1323285372131004416, 1327893319914020864, 1477117401145610240]:\n",
    "    print(group)\n",
    "\n",
    "  iaa_dict = {}                                                   # Initialise a dict for current tweet\n",
    "\n",
    "  for i in range(3):                                              # For each annotator, get their label, i.e. annotated ranges\n",
    "    annotator_label = group.iloc[i]['label']                      # e.g. [[0, 21, 'Happiness'], [128, 142, 'Sadness'], [205, 214, 'Sadness']]\n",
    "\n",
    "    if len(annotator_label) == 0:                                 # If the annotator didn't annotate anything, acknowledge this by setting pseudo index and emotion=('none')\n",
    "      start = -2\n",
    "      end = -1\n",
    "      emotion =  'none'\n",
    "\n",
    "      wholerange = list(range(start, end+1))                      # List of the pseudo range [-2, -1, 0]\n",
    "     \n",
    "      for k in range(len(wholerange)):                            # For each pseudo index number, add the nr and 'none' emotion to the dict\n",
    "        index_num = wholerange[k]                                 # Get current index number, e.g. -2\n",
    "        if index_num in iaa_dict:                                 # If index is already in dict, just add to the emotion label, else add index and emotion label\n",
    "          iaa_dict[index_num].append(emotion)\n",
    "        else:\n",
    "          iaa_dict[index_num] = [emotion]\n",
    "\n",
    "    else:                                                         # If the annotator did annotation something\n",
    "      for j in range(len(annotator_label)):                       # Loop over each annotated range (such as one word) in the sub lists by accessing length (e.g. 3 for the example above)\n",
    "        start = annotator_label[j][0]                             # e.g. 0\n",
    "        end = annotator_label[j][1]                               # e.g. 21\n",
    "        emotion = annotator_label[j][2]                           # e.g. 'Happiness'\n",
    "\n",
    "        wholerange = list(range(start, end+1))                    # List of the range, e.g. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
    "        \n",
    "        for k in range(len(wholerange)):                          # For each index number, add the nr and emotion to the dict\n",
    "          index_num = wholerange[k]\n",
    "          if index_num in iaa_dict:\n",
    "            iaa_dict[index_num].append(emotion)\n",
    "          else:\n",
    "            iaa_dict[index_num] = [emotion]\n",
    "\n",
    "  initial_count = len(iaa_dict)                                   # count before removing all annotations that don't have at least two annotations\n",
    "  total_annotations_count += initial_count\n",
    "  iaa_dict = {k: v for k, v in iaa_dict.items() if len(v) > 1}    # After collecting all annotated range numbers and emotions, drop any key-value pairs with only 1 emotion (= only one annotator marked it)\n",
    "  filtered_count = len(iaa_dict)                                  # count after removing annotations with less than two annotations\n",
    "  dropped_count = initial_count - filtered_count\n",
    "  total_annotations_dropped += dropped_count\n",
    "  \n",
    "  if iaa_dict != {}:                                              # If there are any annotations left \n",
    "    ranges, get_msg_id = find_cont_ranges_and_dom_emotions(iaa_dict, iaa_counts)          # Get the ranges and the dominant emotion\n",
    "    if get_msg_id == 1: \n",
    "      print(message_id)\n",
    "    final_iaa_df.loc[final_iaa_df['message_id'] == message_id, 'label'] = str(ranges)     # add the final ranges and emotions to the 'label' column in the final df\n",
    "  else:                                                           # if there are no emotions then just add empty list brackets in the label column\n",
    "    final_iaa_df.loc[final_iaa_df['message_id'] == message_id, 'label'] = str('[]')\n",
    "\n",
    "# Save to CSV\n",
    "# final_iaa_df.to_csv(\"Data/Training Data/1 - Annotation Results/Final Results/final_iaa_df_new.csv\", index=False)\n",
    "print(f\"Total annotations: {total_annotations_count}\")\n",
    "print(f\"Total annotations dropped: {total_annotations_dropped}\")\n",
    "print(iaa_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total annotations: 50888\n",
      "Total annotations dropped: 5015\n",
      "Ranges with a dominant emotion (iaa_3): 4358\n",
      "Ranges without a dominant emotion (iaa_2): 6\n",
      "Ranges with all different emotions: 0\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters for agreement statistics\n",
    "total_annotations_count = 0\n",
    "total_annotations_dropped = 0\n",
    "all_three_equal_count = 0\n",
    "two_equal_count = 0\n",
    "all_different_count = 0\n",
    "iaa_counts = {'iaa_2': 0, 'iaa_3': 0}\n",
    "\n",
    "# Iterate through groups of message_ids (= 3 annotations per tweet)\n",
    "for message_id, group in sorted_df.groupby('message_id'): \n",
    "    iaa_dict = {}                                                   # Initialize a dict for current tweet\n",
    "\n",
    "    for i in range(3):                                              # For each annotator, get their label, i.e., annotated ranges\n",
    "        annotator_label = group.iloc[i]['label']                    # e.g., [[0, 21, 'Happiness'], [128, 142, 'Sadness'], ...]\n",
    "\n",
    "        if len(annotator_label) == 0:                               # If no annotation, use pseudo range with 'none'\n",
    "            emotion = 'none'\n",
    "            iaa_dict[-2] = iaa_dict.get(-2, []) + [emotion]         # Pseudo range [-2, -1, 0]\n",
    "        else:                                                       # If annotation is present\n",
    "            for j in range(len(annotator_label)):                   # Loop over each annotated range\n",
    "                start, end, emotion = annotator_label[j]\n",
    "                wholerange = list(range(start, end + 1))\n",
    "                \n",
    "                for index_num in wholerange:                        # Add emotion to each index in range\n",
    "                    iaa_dict[index_num] = iaa_dict.get(index_num, []) + [emotion]\n",
    "\n",
    "    # Analyze each continuous range independently\n",
    "    ranges = find_cont_ranges_and_dom_emotions(iaa_dict, iaa_counts)            # Get ranges with dominant emotions\n",
    "\n",
    "    for start, end, dominant_emotion in ranges:\n",
    "        emotions_list = [iaa_dict[i] for i in range(start, end + 1)]\n",
    "        flattened_emotions = [emotion for sublist in emotions_list for emotion in sublist]\n",
    "        emotion_counts = Counter(flattened_emotions)\n",
    "\n",
    "\n",
    "    # Count dropped annotations\n",
    "    initial_count = len(iaa_dict)\n",
    "    total_annotations_count += initial_count\n",
    "    iaa_dict = {k: v for k, v in iaa_dict.items() if len(v) > 1}    # Drop annotations with only one annotator\n",
    "    filtered_count = len(iaa_dict)\n",
    "    dropped_count = initial_count - filtered_count\n",
    "    total_annotations_dropped += dropped_count\n",
    "\n",
    "# Output overall statistics\n",
    "print(f\"Total annotations: {total_annotations_count}\")\n",
    "print(f\"Total annotations dropped: {total_annotations_dropped}\")\n",
    "print(f\"Ranges with a dominant emotion (iaa_3): {iaa_counts['iaa_3']}\")\n",
    "print(f\"Ranges without a dominant emotion (iaa_2): {iaa_counts['iaa_2']}\")\n",
    "print(f\"Ranges with all different emotions: {all_different_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-4byfJqn9UYN",
    "outputId": "ccb7febe-db33-424a-e972-ea95f959fbdb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>message_id</th>\n",
       "      <th>label</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25334</td>\n",
       "      <td>Proud to receive an A+ rating from th Nevada F...</td>\n",
       "      <td>1524255702851620864</td>\n",
       "      <td>[[20, 29, 'Happiness'], [92, 112, 'Happiness']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25335</td>\n",
       "      <td>@AnotherJay @mayor_anderson @BorisJohnson My f...</td>\n",
       "      <td>1323323443446747136</td>\n",
       "      <td>[[225, 235, 'Anger']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25336</td>\n",
       "      <td>I can't even: Gunfire erupts after a high scho...</td>\n",
       "      <td>1532901446114365440</td>\n",
       "      <td>[[14, 21, 'Fear'], [89, 94, 'Sadness']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25337</td>\n",
       "      <td>@codefknblack I had a good gun play session th...</td>\n",
       "      <td>1356204586806222848</td>\n",
       "      <td>[[27, 43, 'Happiness']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25338</td>\n",
       "      <td>@PennySpalpeen @jenfox84 @pastebbins @LucyFan4...</td>\n",
       "      <td>1333825902220881920</td>\n",
       "      <td>[[353, 357, 'Happiness'], [386, 392, 'Happines...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25339</td>\n",
       "      <td>@myhairynipples @SkySportsPL Drogba you prick</td>\n",
       "      <td>1302449973204746240</td>\n",
       "      <td>[[29, 35, 'Anger']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25340</td>\n",
       "      <td>@SandraKobelt @MailOnline A lot more than 13, ...</td>\n",
       "      <td>1524641627184132096</td>\n",
       "      <td>[[-2, -1, 'none']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>25341</td>\n",
       "      <td>Give this a watch well worth it informative an...</td>\n",
       "      <td>1323368847748063232</td>\n",
       "      <td>[[5, 9, 'Happiness']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25342</td>\n",
       "      <td>Can you tell we miss DofE? Getting outside one...</td>\n",
       "      <td>1322993372878020608</td>\n",
       "      <td>[[21, 25, 'Happiness'], [109, 131, 'Happiness']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25343</td>\n",
       "      <td>The @SDSU Police Department and San Diego Fire...</td>\n",
       "      <td>1567966037509025792</td>\n",
       "      <td>[[191, 199, 'Fear']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25344</td>\n",
       "      <td>Wind 7.6 mph SW. Barometer 1004.2 hPa, Falling...</td>\n",
       "      <td>1322973089517445120</td>\n",
       "      <td>[[-2, -1, 'none']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>25345</td>\n",
       "      <td>Bang average, second best and just waiting to ...</td>\n",
       "      <td>1322993272877436928</td>\n",
       "      <td>[[-2, -1, 'none']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25346</td>\n",
       "      <td>@LBC what a load of bollocks. \\nWhoever though...</td>\n",
       "      <td>1311580724454141952</td>\n",
       "      <td>[[0, 4, 'Anger'], [31, 38, 'Anger']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>25347</td>\n",
       "      <td>@_BillieBelieves As a parent that made me cry ...</td>\n",
       "      <td>1302270557673717760</td>\n",
       "      <td>[[0, 16, 'Happiness'], [29, 33, 'Happiness']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>25348</td>\n",
       "      <td>@illi4141 @OborneTweets @oflynnsocial What doe...</td>\n",
       "      <td>1302347000613134336</td>\n",
       "      <td>[[-2, -1, 'none']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>25349</td>\n",
       "      <td>@BandQ Hi, my mother bought a bin and realised...</td>\n",
       "      <td>1356157389297377280</td>\n",
       "      <td>[[-2, -1, 'none']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>25350</td>\n",
       "      <td>For what could quite conceivably be my last co...</td>\n",
       "      <td>1322837240528183296</td>\n",
       "      <td>[[84, 91, 'Happiness'], [109, 119, 'Happiness'...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25351</td>\n",
       "      <td>@whiteycpfc @RedNBlueArmy1 @WhyteleafeEagle @a...</td>\n",
       "      <td>1333870874903121920</td>\n",
       "      <td>[[-2, -1, 'none']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>25352</td>\n",
       "      <td>@premierinn\\nSent you a dm hours ago, any chan...</td>\n",
       "      <td>1302301808208084992</td>\n",
       "      <td>[[0, 11, 'Anger']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>25353</td>\n",
       "      <td>@bike_pat hi there !! I work at @EHSBramley wi...</td>\n",
       "      <td>1333771389757771776</td>\n",
       "      <td>[[0, 9, 'Happiness'], [171, 189, 'Happiness']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>25354</td>\n",
       "      <td>Today is the day! BHFD &amp; BHCPR will be teachin...</td>\n",
       "      <td>1527684131693412352</td>\n",
       "      <td>[[0, 5, 'Happiness'], [65, 70, 'Happiness']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>25355</td>\n",
       "      <td>UK’s Brexit negotiator confirms the government...</td>\n",
       "      <td>1302407703051022336</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25356</td>\n",
       "      <td>@helsmels44 JAG ...highly recommend but this w...</td>\n",
       "      <td>1333721596658380800</td>\n",
       "      <td>[[-2, -1, 'none']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>25357</td>\n",
       "      <td>who can I pay to clean my brushes</td>\n",
       "      <td>1333863349185081344</td>\n",
       "      <td>[[-2, -1, 'none']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25358</td>\n",
       "      <td>how it december ? https://t.co/b7NDy1L769</td>\n",
       "      <td>1333605294635704320</td>\n",
       "      <td>[[-2, -1, 'none']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25359</td>\n",
       "      <td>This one na true??? https://t.co/An5dUOiENG</td>\n",
       "      <td>1356223281645367296</td>\n",
       "      <td>[[-2, -1, 'none']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>25360</td>\n",
       "      <td>Nurse: our #NNMC bill will be placed,\\n\\nStati...</td>\n",
       "      <td>1356221628070432768</td>\n",
       "      <td>[[132, 135, 'Anger']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>25361</td>\n",
       "      <td>@arsenalnewsasit @MKarmy46 You lived! 😂😂😂😂😂😘😘😘</td>\n",
       "      <td>1356203334890369024</td>\n",
       "      <td>[[0, 30, 'Happiness']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>25362</td>\n",
       "      <td>@DLBLLC At its simplest it would be a place fo...</td>\n",
       "      <td>1307599371865858048</td>\n",
       "      <td>[[36, 43, 'Happiness']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>25363</td>\n",
       "      <td>The Uk Government not fit to  run a race.</td>\n",
       "      <td>1322962209748692992</td>\n",
       "      <td>[[4, 17, 'Anger']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>25364</td>\n",
       "      <td>@ooopsididntdidi Thanks, you too 😁x</td>\n",
       "      <td>1311533987693842432</td>\n",
       "      <td>[[0, 16, 'Happiness']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>25365</td>\n",
       "      <td>Tonight on @10News:\\nWe're keeping an eye on t...</td>\n",
       "      <td>1485424150348853248</td>\n",
       "      <td>[[48, 70, 'Fear']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>25366</td>\n",
       "      <td>@British_Airways @JoJoLondon2016 My parents ha...</td>\n",
       "      <td>1311605144132386816</td>\n",
       "      <td>[[0, 32, 'Anger'], [150, 157, 'Anger'], [162, ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>25367</td>\n",
       "      <td>Who Dares Wins....As Delboy would say....😂😁😀😁😁...</td>\n",
       "      <td>1356276061307342848</td>\n",
       "      <td>[[21, 27, 'Happiness'], [66, 68, 'Happiness']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>25368</td>\n",
       "      <td>I hate wildfires. Ever since we had to evacuat...</td>\n",
       "      <td>1565141637801463808</td>\n",
       "      <td>[[7, 16, 'Anger'], [55, 65, 'Anger']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>25369</td>\n",
       "      <td>@JeremyVineOn5 They should serve £0.75p lasagn...</td>\n",
       "      <td>1333720374291406848</td>\n",
       "      <td>[[15, 19, 'Anger'], [72, 82, 'Happiness'], [87...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>25370</td>\n",
       "      <td>@kev_g1 Satin bottom with a glittery top - too...</td>\n",
       "      <td>1323022845421752320</td>\n",
       "      <td>[[8, 40, 'Sadness'], [57, 59, 'Fear']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>25371</td>\n",
       "      <td>@Daveyb_50609 Have you watched utopia?</td>\n",
       "      <td>1333773771518439424</td>\n",
       "      <td>[[-2, -1, 'none']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>25372</td>\n",
       "      <td>@J_Scott_95 I want to be religious, which Chri...</td>\n",
       "      <td>1302305763478638592</td>\n",
       "      <td>[[-2, -1, 'none']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>25373</td>\n",
       "      <td>I think it's in my wrist now too !!!🙈🤨 mutteri...</td>\n",
       "      <td>1322989532275462144</td>\n",
       "      <td>[[8, 10, 'Happiness'], [51, 57, 'Happiness']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>25374</td>\n",
       "      <td>LA-West Structure Fire reported at N el Centro...</td>\n",
       "      <td>1525309487149486080</td>\n",
       "      <td>[[-2, -1, 'none']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>25375</td>\n",
       "      <td>Watching #TheAdamsFamily https://t.co/hntIWsRP2m</td>\n",
       "      <td>1322948059177902080</td>\n",
       "      <td>[[-2, -1, 'none']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>25376</td>\n",
       "      <td>Hever and Chidingstone. @ Hever, Kent https://...</td>\n",
       "      <td>1345404609691611136</td>\n",
       "      <td>[[-2, -1, 'none']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>25377</td>\n",
       "      <td>@CameronYardeJnr feels like November was only ...</td>\n",
       "      <td>1333678412674764800</td>\n",
       "      <td>[[28, 36, 'Happiness']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>25378</td>\n",
       "      <td>3 months and 5 days until we get #ResidentEvil...</td>\n",
       "      <td>1356139806116425728</td>\n",
       "      <td>[[33, 53, 'Happiness']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>25379</td>\n",
       "      <td>@shopgeminidaze @mofthelionart Paint splattere...</td>\n",
       "      <td>1302287338391982080</td>\n",
       "      <td>[[31, 68, 'Happiness']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>25380</td>\n",
       "      <td>We miss ol’ Steve.  We literally wouldn’t be h...</td>\n",
       "      <td>1302356307232653312</td>\n",
       "      <td>[[8, 17, 'Sadness']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>25381</td>\n",
       "      <td>The NEU hasn’t “read the room” well for a whil...</td>\n",
       "      <td>1322880938699870208</td>\n",
       "      <td>[[4, 7, 'Anger']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>25382</td>\n",
       "      <td>@craftsandtools @woodpunk Google which turntab...</td>\n",
       "      <td>1356240914780446720</td>\n",
       "      <td>[[178, 188, 'Fear']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>25383</td>\n",
       "      <td>@aneeqatweets how are you not twitter famous y...</td>\n",
       "      <td>1312043606711119872</td>\n",
       "      <td>[[0, 13, 'Happiness'], [22, 25, 'Happiness'], ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text  \\\n",
       "0   25334  Proud to receive an A+ rating from th Nevada F...   \n",
       "1   25335  @AnotherJay @mayor_anderson @BorisJohnson My f...   \n",
       "2   25336  I can't even: Gunfire erupts after a high scho...   \n",
       "3   25337  @codefknblack I had a good gun play session th...   \n",
       "4   25338  @PennySpalpeen @jenfox84 @pastebbins @LucyFan4...   \n",
       "5   25339      @myhairynipples @SkySportsPL Drogba you prick   \n",
       "6   25340  @SandraKobelt @MailOnline A lot more than 13, ...   \n",
       "7   25341  Give this a watch well worth it informative an...   \n",
       "8   25342  Can you tell we miss DofE? Getting outside one...   \n",
       "9   25343  The @SDSU Police Department and San Diego Fire...   \n",
       "10  25344  Wind 7.6 mph SW. Barometer 1004.2 hPa, Falling...   \n",
       "11  25345  Bang average, second best and just waiting to ...   \n",
       "12  25346  @LBC what a load of bollocks. \\nWhoever though...   \n",
       "13  25347  @_BillieBelieves As a parent that made me cry ...   \n",
       "14  25348  @illi4141 @OborneTweets @oflynnsocial What doe...   \n",
       "15  25349  @BandQ Hi, my mother bought a bin and realised...   \n",
       "16  25350  For what could quite conceivably be my last co...   \n",
       "17  25351  @whiteycpfc @RedNBlueArmy1 @WhyteleafeEagle @a...   \n",
       "18  25352  @premierinn\\nSent you a dm hours ago, any chan...   \n",
       "19  25353  @bike_pat hi there !! I work at @EHSBramley wi...   \n",
       "20  25354  Today is the day! BHFD & BHCPR will be teachin...   \n",
       "21  25355  UK’s Brexit negotiator confirms the government...   \n",
       "22  25356  @helsmels44 JAG ...highly recommend but this w...   \n",
       "23  25357                  who can I pay to clean my brushes   \n",
       "24  25358          how it december ? https://t.co/b7NDy1L769   \n",
       "25  25359        This one na true??? https://t.co/An5dUOiENG   \n",
       "26  25360  Nurse: our #NNMC bill will be placed,\\n\\nStati...   \n",
       "27  25361     @arsenalnewsasit @MKarmy46 You lived! 😂😂😂😂😂😘😘😘   \n",
       "28  25362  @DLBLLC At its simplest it would be a place fo...   \n",
       "29  25363          The Uk Government not fit to  run a race.   \n",
       "30  25364                @ooopsididntdidi Thanks, you too 😁x   \n",
       "31  25365  Tonight on @10News:\\nWe're keeping an eye on t...   \n",
       "32  25366  @British_Airways @JoJoLondon2016 My parents ha...   \n",
       "33  25367  Who Dares Wins....As Delboy would say....😂😁😀😁😁...   \n",
       "34  25368  I hate wildfires. Ever since we had to evacuat...   \n",
       "35  25369  @JeremyVineOn5 They should serve £0.75p lasagn...   \n",
       "36  25370  @kev_g1 Satin bottom with a glittery top - too...   \n",
       "37  25371             @Daveyb_50609 Have you watched utopia?   \n",
       "38  25372  @J_Scott_95 I want to be religious, which Chri...   \n",
       "39  25373  I think it's in my wrist now too !!!🙈🤨 mutteri...   \n",
       "40  25374  LA-West Structure Fire reported at N el Centro...   \n",
       "41  25375   Watching #TheAdamsFamily https://t.co/hntIWsRP2m   \n",
       "42  25376  Hever and Chidingstone. @ Hever, Kent https://...   \n",
       "43  25377  @CameronYardeJnr feels like November was only ...   \n",
       "44  25378  3 months and 5 days until we get #ResidentEvil...   \n",
       "45  25379  @shopgeminidaze @mofthelionart Paint splattere...   \n",
       "46  25380  We miss ol’ Steve.  We literally wouldn’t be h...   \n",
       "47  25381  The NEU hasn’t “read the room” well for a whil...   \n",
       "48  25382  @craftsandtools @woodpunk Google which turntab...   \n",
       "49  25383  @aneeqatweets how are you not twitter famous y...   \n",
       "\n",
       "             message_id                                              label  \\\n",
       "0   1524255702851620864    [[20, 29, 'Happiness'], [92, 112, 'Happiness']]   \n",
       "1   1323323443446747136                              [[225, 235, 'Anger']]   \n",
       "2   1532901446114365440            [[14, 21, 'Fear'], [89, 94, 'Sadness']]   \n",
       "3   1356204586806222848                            [[27, 43, 'Happiness']]   \n",
       "4   1333825902220881920  [[353, 357, 'Happiness'], [386, 392, 'Happines...   \n",
       "5   1302449973204746240                                [[29, 35, 'Anger']]   \n",
       "6   1524641627184132096                                 [[-2, -1, 'none']]   \n",
       "7   1323368847748063232                              [[5, 9, 'Happiness']]   \n",
       "8   1322993372878020608   [[21, 25, 'Happiness'], [109, 131, 'Happiness']]   \n",
       "9   1567966037509025792                               [[191, 199, 'Fear']]   \n",
       "10  1322973089517445120                                 [[-2, -1, 'none']]   \n",
       "11  1322993272877436928                                 [[-2, -1, 'none']]   \n",
       "12  1311580724454141952               [[0, 4, 'Anger'], [31, 38, 'Anger']]   \n",
       "13  1302270557673717760      [[0, 16, 'Happiness'], [29, 33, 'Happiness']]   \n",
       "14  1302347000613134336                                 [[-2, -1, 'none']]   \n",
       "15  1356157389297377280                                 [[-2, -1, 'none']]   \n",
       "16  1322837240528183296  [[84, 91, 'Happiness'], [109, 119, 'Happiness'...   \n",
       "17  1333870874903121920                                 [[-2, -1, 'none']]   \n",
       "18  1302301808208084992                                 [[0, 11, 'Anger']]   \n",
       "19  1333771389757771776     [[0, 9, 'Happiness'], [171, 189, 'Happiness']]   \n",
       "20  1527684131693412352       [[0, 5, 'Happiness'], [65, 70, 'Happiness']]   \n",
       "21  1302407703051022336                                                 []   \n",
       "22  1333721596658380800                                 [[-2, -1, 'none']]   \n",
       "23  1333863349185081344                                 [[-2, -1, 'none']]   \n",
       "24  1333605294635704320                                 [[-2, -1, 'none']]   \n",
       "25  1356223281645367296                                 [[-2, -1, 'none']]   \n",
       "26  1356221628070432768                              [[132, 135, 'Anger']]   \n",
       "27  1356203334890369024                             [[0, 30, 'Happiness']]   \n",
       "28  1307599371865858048                            [[36, 43, 'Happiness']]   \n",
       "29  1322962209748692992                                 [[4, 17, 'Anger']]   \n",
       "30  1311533987693842432                             [[0, 16, 'Happiness']]   \n",
       "31  1485424150348853248                                 [[48, 70, 'Fear']]   \n",
       "32  1311605144132386816  [[0, 32, 'Anger'], [150, 157, 'Anger'], [162, ...   \n",
       "33  1356276061307342848     [[21, 27, 'Happiness'], [66, 68, 'Happiness']]   \n",
       "34  1565141637801463808              [[7, 16, 'Anger'], [55, 65, 'Anger']]   \n",
       "35  1333720374291406848  [[15, 19, 'Anger'], [72, 82, 'Happiness'], [87...   \n",
       "36  1323022845421752320             [[8, 40, 'Sadness'], [57, 59, 'Fear']]   \n",
       "37  1333773771518439424                                 [[-2, -1, 'none']]   \n",
       "38  1302305763478638592                                 [[-2, -1, 'none']]   \n",
       "39  1322989532275462144      [[8, 10, 'Happiness'], [51, 57, 'Happiness']]   \n",
       "40  1525309487149486080                                 [[-2, -1, 'none']]   \n",
       "41  1322948059177902080                                 [[-2, -1, 'none']]   \n",
       "42  1345404609691611136                                 [[-2, -1, 'none']]   \n",
       "43  1333678412674764800                            [[28, 36, 'Happiness']]   \n",
       "44  1356139806116425728                            [[33, 53, 'Happiness']]   \n",
       "45  1302287338391982080                            [[31, 68, 'Happiness']]   \n",
       "46  1302356307232653312                               [[8, 17, 'Sadness']]   \n",
       "47  1322880938699870208                                  [[4, 7, 'Anger']]   \n",
       "48  1356240914780446720                               [[178, 188, 'Fear']]   \n",
       "49  1312043606711119872  [[0, 13, 'Happiness'], [22, 25, 'Happiness'], ...   \n",
       "\n",
       "   Comments  \n",
       "0        []  \n",
       "1        []  \n",
       "2        []  \n",
       "3        []  \n",
       "4        []  \n",
       "5        []  \n",
       "6        []  \n",
       "7        []  \n",
       "8        []  \n",
       "9        []  \n",
       "10       []  \n",
       "11       []  \n",
       "12       []  \n",
       "13       []  \n",
       "14       []  \n",
       "15       []  \n",
       "16       []  \n",
       "17       []  \n",
       "18       []  \n",
       "19       []  \n",
       "20       []  \n",
       "21       []  \n",
       "22       []  \n",
       "23       []  \n",
       "24       []  \n",
       "25       []  \n",
       "26       []  \n",
       "27       []  \n",
       "28       []  \n",
       "29       []  \n",
       "30       []  \n",
       "31       []  \n",
       "32       []  \n",
       "33       []  \n",
       "34       []  \n",
       "35       []  \n",
       "36       []  \n",
       "37       []  \n",
       "38       []  \n",
       "39       []  \n",
       "40       []  \n",
       "41       []  \n",
       "42       []  \n",
       "43       []  \n",
       "44       []  \n",
       "45       []  \n",
       "46       []  \n",
       "47       []  \n",
       "48       []  \n",
       "49       []  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_iaa_df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Added: Re-Load Final Dataset for further Data-Cleaning and Subsetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2766\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>message_id</th>\n",
       "      <th>label</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25334</td>\n",
       "      <td>Proud to receive an A+ rating from th Nevada F...</td>\n",
       "      <td>1524255702851620864</td>\n",
       "      <td>[[20, 29, 'Happiness'], [92, 112, 'Happiness']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25335</td>\n",
       "      <td>@AnotherJay @mayor_anderson @BorisJohnson My f...</td>\n",
       "      <td>1323323443446747136</td>\n",
       "      <td>[[225, 235, 'Anger']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25336</td>\n",
       "      <td>I can't even: Gunfire erupts after a high scho...</td>\n",
       "      <td>1532901446114365440</td>\n",
       "      <td>[[14, 21, 'Fear'], [89, 94, 'Sadness']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25337</td>\n",
       "      <td>@codefknblack I had a good gun play session th...</td>\n",
       "      <td>1356204586806222848</td>\n",
       "      <td>[[27, 43, 'Happiness']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25338</td>\n",
       "      <td>@PennySpalpeen @jenfox84 @pastebbins @LucyFan4...</td>\n",
       "      <td>1333825902220881920</td>\n",
       "      <td>[[353, 357, 'Happiness'], [386, 392, 'Happines...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text  \\\n",
       "0  25334  Proud to receive an A+ rating from th Nevada F...   \n",
       "1  25335  @AnotherJay @mayor_anderson @BorisJohnson My f...   \n",
       "2  25336  I can't even: Gunfire erupts after a high scho...   \n",
       "3  25337  @codefknblack I had a good gun play session th...   \n",
       "4  25338  @PennySpalpeen @jenfox84 @pastebbins @LucyFan4...   \n",
       "\n",
       "            message_id                                              label  \\\n",
       "0  1524255702851620864    [[20, 29, 'Happiness'], [92, 112, 'Happiness']]   \n",
       "1  1323323443446747136                              [[225, 235, 'Anger']]   \n",
       "2  1532901446114365440            [[14, 21, 'Fear'], [89, 94, 'Sadness']]   \n",
       "3  1356204586806222848                            [[27, 43, 'Happiness']]   \n",
       "4  1333825902220881920  [[353, 357, 'Happiness'], [386, 392, 'Happines...   \n",
       "\n",
       "  Comments  \n",
       "0       []  \n",
       "1       []  \n",
       "2       []  \n",
       "3       []  \n",
       "4       []  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"Data/Training Data/1 - Annotation Results/Final Results/final_iaa_df_new.csv\")\n",
    "print(len(df))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Reformat final Training Dataset for GRACE Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input data format example: \n",
    "\n",
    "> <b>Text</b>: Proud to receive an A+ rating from th Nevada Firearms Coalition. I will ALWAYS defend your 2nd amendment rights!\n",
    ">\n",
    "> <b>Label</b>: [[20, 29, 'Happiness'], [92, 112, 'Happiness']]\n",
    "\n",
    "\n",
    "Output format example:\n",
    "\n",
    "Proud - - O O O <br>\n",
    "to - - O O O <br>\n",
    "receive - - O O O <br>\n",
    "an  - - O O O <br>\n",
    "A+ - - B_AP HAPPINESS B_AP+HAPPINESS <br>\n",
    "rating - - I_AP HAPPINESS I_AP+HAPPINESS <br>\n",
    "from - - O O O <br>\n",
    "th - - O O O <br>\n",
    "Nevada - - O O O <br>\n",
    "Firearms - - O O O <br>\n",
    "Coalition - - O O O <br>\n",
    ". - - O O O <br>\n",
    "I - - O O O <br>\n",
    "will - - O O O <br>\n",
    "ALWAYS - - O O O <br>\n",
    "defend - - O O O <br>\n",
    "your - - O O O <br>\n",
    "2nd - - B_AP HAPPINESS B_AP+HAPPINESS <br>\n",
    "amendment - - I_AP HAPPINESS I_AP+HAPPINESS <br>\n",
    "rights - - I_AP HAPPINESS I_AP+HAPPINESS <br>\n",
    "! - - O O O <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2766\n",
      "2212\n",
      "554\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>message_id</th>\n",
       "      <th>label</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>8994</td>\n",
       "      <td>Homeless camp fires are a regular thing. Light...</td>\n",
       "      <td>1531923081886769152</td>\n",
       "      <td>[[41, 84, 'Anger']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1666</th>\n",
       "      <td>8396</td>\n",
       "      <td>https://t.co/GvLdJyIvcp - Quite frankly @Pasto...</td>\n",
       "      <td>1305950140658855936</td>\n",
       "      <td>[[40, 59, 'Anger']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>461</td>\n",
       "      <td>Just posted a video @ Blackheath, London https...</td>\n",
       "      <td>1311576244899717120</td>\n",
       "      <td>[[-2, -1, 'none']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>1009</td>\n",
       "      <td>@Disney should do a #starwars what if scenario...</td>\n",
       "      <td>1302275358339858432</td>\n",
       "      <td>[[0, 7, 'Happiness'], [20, 46, 'Happiness'], [...</td>\n",
       "      <td>['example for compound targets']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>694</td>\n",
       "      <td>Thought Mall looked wonderfully dilapidated in...</td>\n",
       "      <td>1302268394738155520</td>\n",
       "      <td>[[8, 12, 'Sadness']]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "2263  8994  Homeless camp fires are a regular thing. Light...   \n",
       "1666  8396  https://t.co/GvLdJyIvcp - Quite frankly @Pasto...   \n",
       "412    461  Just posted a video @ Blackheath, London https...   \n",
       "960   1009  @Disney should do a #starwars what if scenario...   \n",
       "645    694  Thought Mall looked wonderfully dilapidated in...   \n",
       "\n",
       "               message_id                                              label  \\\n",
       "2263  1531923081886769152                                [[41, 84, 'Anger']]   \n",
       "1666  1305950140658855936                                [[40, 59, 'Anger']]   \n",
       "412   1311576244899717120                                 [[-2, -1, 'none']]   \n",
       "960   1302275358339858432  [[0, 7, 'Happiness'], [20, 46, 'Happiness'], [...   \n",
       "645   1302268394738155520                               [[8, 12, 'Sadness']]   \n",
       "\n",
       "                              Comments  \n",
       "2263                                []  \n",
       "1666                                []  \n",
       "412                                 []  \n",
       "960   ['example for compound targets']  \n",
       "645                                 []  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df))\n",
    "\n",
    "\n",
    "# Split the DataFrame into training and test sets\n",
    "# 'test_size' is set to 0.15 for 15% test data and 85% training data\n",
    "# 'random_state' can be set to an integer for reproducibility\n",
    "\n",
    "\n",
    "train_df, test_df = train_test_split(final_iaa_df, test_size=0.1, random_state=42)\n",
    "# train_df, test_df = train_test_split(df, test_size=0.2, random_state=38)\n",
    "\n",
    "print(len(train_df))\n",
    "print(len(test_df))\n",
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text, labels):\n",
    "    words = text.split()\n",
    "    indices = []\n",
    "    start = 0\n",
    "    for word in words:\n",
    "        end = start + len(word)\n",
    "        indices.append((start, end))\n",
    "        start = end + 1  # +1 for the space or newline character\n",
    "\n",
    "    formatted_text = []\n",
    "    # reformat the lists which are stored as strings back into list formats\n",
    "    labels = ast.literal_eval(labels)\n",
    "\n",
    "    for i, (word, (start_idx, end_idx)) in enumerate(zip(words, indices)):\n",
    "        label = \"O O O\"\n",
    "        for label_range in labels:\n",
    "            # a start index of -2 indicates that the label is \"none\"\n",
    "            if start == -2:\n",
    "                label = \"O O O\"\n",
    "            else:\n",
    "                range_start, range_end, emotion = label_range\n",
    "                if start_idx >= range_start and end_idx <= range_end:\n",
    "                    if start_idx == range_start:\n",
    "                        label = f\"B_AP {emotion.upper()} B_AP+{emotion.upper()}\"\n",
    "                    else:\n",
    "                        label = f\"I_AP {emotion.upper()} I_AP+{emotion.upper()}\"\n",
    "                    break\n",
    "        formatted_text.append(f\"{word} - - {label}\")\n",
    "\n",
    "    return formatted_text\n",
    "\n",
    "def format_dataframe(df):\n",
    "    all_formatted_texts = []\n",
    "    for _, row in df.iterrows():\n",
    "        formatted_text = process_text(row['text'], row['label'])\n",
    "        all_formatted_texts.extend(formatted_text)\n",
    "        all_formatted_texts.append(\"\")  # Empty row between tweets\n",
    "\n",
    "    return \"\\n\".join(all_formatted_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whole dataset reformatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output written to Data/Training Data/2 - Data Reformatted and Split for Training/abea_training_dataset.txt\n"
     ]
    }
   ],
   "source": [
    "# Process the DataFrame\n",
    "formatted_data = format_dataframe(final_iaa_df)\n",
    "\n",
    "# Write the output to a .txt file\n",
    "output_file_path = \"Data/Training Data/2 - Data Reformatted and Split for Training/abea_training_dataset.txt\"\n",
    "with open(output_file_path, 'w') as file:\n",
    "    file.write('-DOCSTART-\\n\\n')\n",
    "    file.write(formatted_data)\n",
    "\n",
    "print(f\"Output written to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and Test Datasets Reformatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output written to Data/Training Data/2 - Data Reformatted and Split for Training/abea_train.txt\n",
      "Output written to Data/Training Data/2 - Data Reformatted and Split for Training/abea_trial.txt\n",
      "Output written to Data/Training Data/2 - Data Reformatted and Split for Training/abea_test.gold.txt\n"
     ]
    }
   ],
   "source": [
    "# Process the DataFrame\n",
    "formatted_data = format_dataframe(train_df)\n",
    "# Write the output to a .txt file\n",
    "training_file_paths = [\"Data/Training Data/2 - Data Reformatted and Split for Training/abea_resplit_train.txt\", \n",
    "                       \"Data/Training Data/2 - Data Reformatted and Split for Training/abea__resplit_trial.txt\"]\n",
    "for path in training_file_paths:\n",
    "    with open(path, 'w') as file:\n",
    "        file.write('-DOCSTART-\\n\\n')\n",
    "        file.write(formatted_data)\n",
    "\n",
    "    print(f\"Output written to {path}\")\n",
    "\n",
    "\n",
    "\n",
    "# Process the DataFrame\n",
    "formatted_data = format_dataframe(test_df)\n",
    "# Write the output to a .txt file\n",
    "output_file_path = \"Data/Training Data/2 - Data Reformatted and Split for Training/abea_test.gold.txt\"\n",
    "with open(output_file_path, 'w') as file:\n",
    "    file.write('-DOCSTART-\\n\\n')\n",
    "    file.write(formatted_data)\n",
    "\n",
    "print(f\"Output written to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing of Refined Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2766\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>message_id</th>\n",
       "      <th>label</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2767</td>\n",
       "      <td>Proud to receive an A+ rating from th Nevada F...</td>\n",
       "      <td>1524255702851620864</td>\n",
       "      <td>[[20, 29, pride], [87, 112, pride]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2768</td>\n",
       "      <td>@AnotherJay @mayor_anderson @BorisJohnson My f...</td>\n",
       "      <td>1323323443446747136</td>\n",
       "      <td>[[57, 62, irritation], [225, 235, frustration]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2769</td>\n",
       "      <td>I can't even: Gunfire erupts after a high scho...</td>\n",
       "      <td>1532901446114365440</td>\n",
       "      <td>[[14, 21, disappointment], [81, 94, sadness]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2770</td>\n",
       "      <td>@codefknblack I had a good gun play session th...</td>\n",
       "      <td>1356204586806222848</td>\n",
       "      <td>[[27, 43, excitement]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2771</td>\n",
       "      <td>@PennySpalpeen @jenfox84 @pastebbins @LucyFan4...</td>\n",
       "      <td>1333825902220881920</td>\n",
       "      <td>[[348, 352, affection], [408, 413, cheerfulness]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text  \\\n",
       "0  2767  Proud to receive an A+ rating from th Nevada F...   \n",
       "1  2768  @AnotherJay @mayor_anderson @BorisJohnson My f...   \n",
       "2  2769  I can't even: Gunfire erupts after a high scho...   \n",
       "3  2770  @codefknblack I had a good gun play session th...   \n",
       "4  2771  @PennySpalpeen @jenfox84 @pastebbins @LucyFan4...   \n",
       "\n",
       "            message_id                                              label  \\\n",
       "0  1524255702851620864                [[20, 29, pride], [87, 112, pride]]   \n",
       "1  1323323443446747136    [[57, 62, irritation], [225, 235, frustration]]   \n",
       "2  1532901446114365440      [[14, 21, disappointment], [81, 94, sadness]]   \n",
       "3  1356204586806222848                             [[27, 43, excitement]]   \n",
       "4  1333825902220881920  [[348, 352, affection], [408, 413, cheerfulness]]   \n",
       "\n",
       "  Comments  \n",
       "0       []  \n",
       "1       []  \n",
       "2       []  \n",
       "3       []  \n",
       "4       []  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load only one file\n",
    "df = pd.read_json(\"Data/Training Data/Refined Annotations/aspect-emotion.jsonl\", lines=True)\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2625\n"
     ]
    }
   ],
   "source": [
    "# Define words that indicate rows to remove\n",
    "# words_to_remove = [\"remove\", \"food ad\", \"tricky\", \"grammar\"]\n",
    "words_to_remove = [\"remove\", \"tricky\", \"grammar\"]\n",
    "\n",
    "# Function to check if any word in the list is in the comments\n",
    "def filter_comments(comment_list):\n",
    "    return not any(word in comment_list for word in words_to_remove)\n",
    "\n",
    "# Filter rows based on comments\n",
    "df = df[df['Comments'].apply(filter_comments)]\n",
    "\n",
    "print(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>message_id</th>\n",
       "      <th>label</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2767</td>\n",
       "      <td>Proud to receive an A+ rating from th Nevada F...</td>\n",
       "      <td>1524255702851620864</td>\n",
       "      <td>[[20, 29, Happiness], [87, 112, Happiness]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2768</td>\n",
       "      <td>@AnotherJay @mayor_anderson @BorisJohnson My f...</td>\n",
       "      <td>1323323443446747136</td>\n",
       "      <td>[[57, 62, Anger], [225, 235, Anger]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2769</td>\n",
       "      <td>I can't even: Gunfire erupts after a high scho...</td>\n",
       "      <td>1532901446114365440</td>\n",
       "      <td>[[14, 21, Sadness], [81, 94, Sadness]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2770</td>\n",
       "      <td>@codefknblack I had a good gun play session th...</td>\n",
       "      <td>1356204586806222848</td>\n",
       "      <td>[[27, 43, Happiness]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2771</td>\n",
       "      <td>@PennySpalpeen @jenfox84 @pastebbins @LucyFan4...</td>\n",
       "      <td>1333825902220881920</td>\n",
       "      <td>[[348, 352, Happiness], [408, 413, Happiness]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2772</td>\n",
       "      <td>@myhairynipples @SkySportsPL Drogba you prick</td>\n",
       "      <td>1302449973204746240</td>\n",
       "      <td>[[29, 35, Anger]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2773</td>\n",
       "      <td>@SandraKobelt @MailOnline A lot more than 13, ...</td>\n",
       "      <td>1524641627184132096</td>\n",
       "      <td>[[50, 74, None]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2774</td>\n",
       "      <td>Give this a watch well worth it informative an...</td>\n",
       "      <td>1323368847748063232</td>\n",
       "      <td>[[5, 9, Happiness]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2775</td>\n",
       "      <td>Can you tell we miss DofE? Getting outside one...</td>\n",
       "      <td>1322993372878020608</td>\n",
       "      <td>[[21, 25, Happiness], [109, 131, Happiness]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2776</td>\n",
       "      <td>The @SDSU Police Department and San Diego Fire...</td>\n",
       "      <td>1567966037509025792</td>\n",
       "      <td>[[78, 90, None]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text  \\\n",
       "0  2767  Proud to receive an A+ rating from th Nevada F...   \n",
       "1  2768  @AnotherJay @mayor_anderson @BorisJohnson My f...   \n",
       "2  2769  I can't even: Gunfire erupts after a high scho...   \n",
       "3  2770  @codefknblack I had a good gun play session th...   \n",
       "4  2771  @PennySpalpeen @jenfox84 @pastebbins @LucyFan4...   \n",
       "5  2772      @myhairynipples @SkySportsPL Drogba you prick   \n",
       "6  2773  @SandraKobelt @MailOnline A lot more than 13, ...   \n",
       "7  2774  Give this a watch well worth it informative an...   \n",
       "8  2775  Can you tell we miss DofE? Getting outside one...   \n",
       "9  2776  The @SDSU Police Department and San Diego Fire...   \n",
       "\n",
       "            message_id                                           label  \\\n",
       "0  1524255702851620864     [[20, 29, Happiness], [87, 112, Happiness]]   \n",
       "1  1323323443446747136            [[57, 62, Anger], [225, 235, Anger]]   \n",
       "2  1532901446114365440          [[14, 21, Sadness], [81, 94, Sadness]]   \n",
       "3  1356204586806222848                           [[27, 43, Happiness]]   \n",
       "4  1333825902220881920  [[348, 352, Happiness], [408, 413, Happiness]]   \n",
       "5  1302449973204746240                               [[29, 35, Anger]]   \n",
       "6  1524641627184132096                                [[50, 74, None]]   \n",
       "7  1323368847748063232                             [[5, 9, Happiness]]   \n",
       "8  1322993372878020608    [[21, 25, Happiness], [109, 131, Happiness]]   \n",
       "9  1567966037509025792                                [[78, 90, None]]   \n",
       "\n",
       "  Comments  \n",
       "0       []  \n",
       "1       []  \n",
       "2       []  \n",
       "3       []  \n",
       "4       []  \n",
       "5       []  \n",
       "6       []  \n",
       "7       []  \n",
       "8       []  \n",
       "9       []  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change labels into the overarching labels\n",
    "# Define the label mappings\n",
    "emotion_mappings = {\n",
    "    'affection': 'Happiness', 'lust': 'Happiness', 'cheerfulness': 'Happiness',\n",
    "    'excitement': 'Happiness', 'contentment': 'Happiness', 'pride': 'Happiness',\n",
    "    'optimism': 'Happiness', 'enthrallment': 'Happiness', 'relief': 'Happiness',\n",
    "    'sarcasm - pos': 'Happiness', 'irritation': 'Anger', 'frustration': 'Anger',\n",
    "    'rage': 'Anger', 'digust': 'Anger', 'envy': 'Anger', 'torment': 'Anger',\n",
    "    'sarcasm - neg': 'Anger', 'suffering': 'Sadness', 'sadness': 'Sadness',\n",
    "    'disappointment': 'Sadness', 'shame': 'Sadness', 'neglect': 'Sadness',\n",
    "    'sympathy': 'Sadness', 'horror': 'Fear', 'nervousness': 'Fear',\n",
    "    'neutral': 'None', 'surprise': 'None'\n",
    "}\n",
    "\n",
    "# Function to remap labels\n",
    "def remap_labels_emotions(labels):\n",
    "    try:\n",
    "        # label_list = ast.literal_eval(label_str)  # Convert the string representation of the list into an actual list\n",
    "        # print(labels)\n",
    "        new_labels = []\n",
    "        for label in labels:\n",
    "            # print(label[2])\n",
    "            if label[2] in emotion_mappings:\n",
    "                new_labels.append([label[0], label[1], emotion_mappings[label[2]]])\n",
    "        return new_labels\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {labels}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Apply the remapping function to the label column\n",
    "df['label'] = df['label'].apply(remap_labels_emotions)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>message_id</th>\n",
       "      <th>label</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2767</td>\n",
       "      <td>Proud to receive an A+ rating from th Nevada F...</td>\n",
       "      <td>1524255702851620864</td>\n",
       "      <td>[[20, 29, Positive], [87, 112, Positive]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2768</td>\n",
       "      <td>@AnotherJay @mayor_anderson @BorisJohnson My f...</td>\n",
       "      <td>1323323443446747136</td>\n",
       "      <td>[[57, 62, Negative], [225, 235, Negative]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2769</td>\n",
       "      <td>I can't even: Gunfire erupts after a high scho...</td>\n",
       "      <td>1532901446114365440</td>\n",
       "      <td>[[14, 21, Negative], [81, 94, Negative]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2770</td>\n",
       "      <td>@codefknblack I had a good gun play session th...</td>\n",
       "      <td>1356204586806222848</td>\n",
       "      <td>[[27, 43, Positive]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2771</td>\n",
       "      <td>@PennySpalpeen @jenfox84 @pastebbins @LucyFan4...</td>\n",
       "      <td>1333825902220881920</td>\n",
       "      <td>[[348, 352, Positive], [408, 413, Positive]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2772</td>\n",
       "      <td>@myhairynipples @SkySportsPL Drogba you prick</td>\n",
       "      <td>1302449973204746240</td>\n",
       "      <td>[[29, 35, Negative]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2773</td>\n",
       "      <td>@SandraKobelt @MailOnline A lot more than 13, ...</td>\n",
       "      <td>1524641627184132096</td>\n",
       "      <td>[[50, 74, Neutral]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2774</td>\n",
       "      <td>Give this a watch well worth it informative an...</td>\n",
       "      <td>1323368847748063232</td>\n",
       "      <td>[[5, 9, Positive]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2775</td>\n",
       "      <td>Can you tell we miss DofE? Getting outside one...</td>\n",
       "      <td>1322993372878020608</td>\n",
       "      <td>[[21, 25, Positive], [109, 131, Positive]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2776</td>\n",
       "      <td>The @SDSU Police Department and San Diego Fire...</td>\n",
       "      <td>1567966037509025792</td>\n",
       "      <td>[[78, 90, Neutral]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text  \\\n",
       "0  2767  Proud to receive an A+ rating from th Nevada F...   \n",
       "1  2768  @AnotherJay @mayor_anderson @BorisJohnson My f...   \n",
       "2  2769  I can't even: Gunfire erupts after a high scho...   \n",
       "3  2770  @codefknblack I had a good gun play session th...   \n",
       "4  2771  @PennySpalpeen @jenfox84 @pastebbins @LucyFan4...   \n",
       "5  2772      @myhairynipples @SkySportsPL Drogba you prick   \n",
       "6  2773  @SandraKobelt @MailOnline A lot more than 13, ...   \n",
       "7  2774  Give this a watch well worth it informative an...   \n",
       "8  2775  Can you tell we miss DofE? Getting outside one...   \n",
       "9  2776  The @SDSU Police Department and San Diego Fire...   \n",
       "\n",
       "            message_id                                         label Comments  \n",
       "0  1524255702851620864     [[20, 29, Positive], [87, 112, Positive]]       []  \n",
       "1  1323323443446747136    [[57, 62, Negative], [225, 235, Negative]]       []  \n",
       "2  1532901446114365440      [[14, 21, Negative], [81, 94, Negative]]       []  \n",
       "3  1356204586806222848                          [[27, 43, Positive]]       []  \n",
       "4  1333825902220881920  [[348, 352, Positive], [408, 413, Positive]]       []  \n",
       "5  1302449973204746240                          [[29, 35, Negative]]       []  \n",
       "6  1524641627184132096                           [[50, 74, Neutral]]       []  \n",
       "7  1323368847748063232                            [[5, 9, Positive]]       []  \n",
       "8  1322993372878020608    [[21, 25, Positive], [109, 131, Positive]]       []  \n",
       "9  1567966037509025792                           [[78, 90, Neutral]]       []  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change labels into the overarching labels\n",
    "# Define the label mappings\n",
    "sentiment_mappings = {\n",
    "    'Happiness': 'Positive', 'Anger': 'Negative', 'Sadness': 'Negative',\n",
    "    'Fear': 'Negative', 'None': 'Neutral'\n",
    "}\n",
    "\n",
    "# Function to remap labels\n",
    "def remap_labels_sentiments(labels):\n",
    "    try:\n",
    "        # label_list = ast.literal_eval(label_str)  # Convert the string representation of the list into an actual list\n",
    "        # print(labels)\n",
    "        new_labels = []\n",
    "        for label in labels:\n",
    "            # print(label[2])\n",
    "            if label[2] in sentiment_mappings:\n",
    "                new_labels.append([label[0], label[1], sentiment_mappings[label[2]]])\n",
    "        return new_labels\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {labels}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Apply the remapping function to the label column\n",
    "df['label'] = df['label'].apply(remap_labels_sentiments)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "df.to_csv(\"Data/Training Data/Refined Annotations/prepared_sentiments.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "8K1ft1jPx8df"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "absa-application",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
